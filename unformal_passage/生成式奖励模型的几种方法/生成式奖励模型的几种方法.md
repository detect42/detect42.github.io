# <center> 生成式奖励模型的几种方法 </center>
[简要讨论 link](https://blog.csdn.net/fjfdg666/article/details/143969076)

列出的这几篇论文是**2024年最新的生成式奖励模型（Generative Reward Models, GRM）方向的重要工作**。下面我会**逐条为你展开讲解**每一篇的核心思路、方法结构、创新点和关键词，

---

## 🧠 生成式奖励模型（GRM）背景介绍

传统奖励模型（如 InstructGPT 使用的）大多是：
- 在 LLM 后面加一个 MLP（value head）
- 输出一个 scalar 值作为 reward
- 通常使用 pairwise 偏好数据 + ranking loss 或 DPO 来训练

但这些模型有几个问题：
- **不可解释**（只是一个分数，没有理由）
- **结构封闭**（需要改模型结构）
- **鲁棒性差**（容易受样本偏差影响）

于是最近兴起了一类新的范式：**生成式奖励模型（Generative Reward Model）**，即：
> 利用 LLM 原有的生成能力，输出语言化的评价（评语、判断、Yes/No、分数等），作为奖励模型。

---

## 1️⃣ **Beyond Scalar Reward Model: Learning Generative Judge from Preference Data**
📅 2024.10 | 📌 方法名：**Con-J**

### 📌 核心思想：
- 不再输出一个 scalar（分数）
- 而是让 LLM 直接生成两段文字评价（正/负）
- 利用这两个生成内容之间的对比，来训练模型进行偏好判断

### ✅ 方法亮点：
- **Con-J（Contrastive Judgement）**：对同一输入的两个回答生成评价 → 形成偏好对比信号
- 用 DPO loss 来训练，使模型更倾向于偏好更好的回答
- 输出带理由的判断，**可解释性强**
- 对训练数据中的偏见（如模板化表达）更鲁棒

### 🧠 总结：
> Con-J 用语言对比来替代 scalar loss，让模型“说出它为什么喜欢 A 不喜欢 B”。

---

## 2️⃣ **Generative Verifiers: Reward Modeling as Next-Token Prediction**
📅 2024.10

### 📌 核心思想：
- 把“奖励建模”当成语言模型的“补全任务”
- 给 prompt 和 response 后面加上一句话：
  ```
  Is the answer correct (Yes/No)?
  ```
- 然后看模型预测 “Yes” 的概率是多少 → 作为奖励值

### ✅ 方法亮点：
- **不改模型结构**，只用原始语言建模能力
- 可视为“prompt 编程 + next-token 概率预测”
- 支持 SFT 和 CoT-SFT 两种训练方式

### 🧠 总结：
> 这是把 reward modeling 转换为一个非常自然的“语言建模任务”，实现简单，泛化强。

---

## 3️⃣ **Generative Reward Models**
📅 2024.10 | 📌 方法名：**GenRM / CoT-GenRM**

### 📌 核心思想：
- 提出了 GenRM：在不加 value head 的前提下，用 “预测 Yes 的概率” 作为 reward
- 使用 DPO loss 进行训练，模型更自然地偏好人类喜欢的回答

### 📌 CoT-GenRM 扩展：
- 在回答前引入 Chain-of-Thought 思维链
- 用 CoT 作为偏好依据，引导模型理解为什么一个回答更好
- 推理时先生成 CoT，再判断偏好

### ✅ 方法亮点：
- 完全不改结构，**即插即用**
- 引入 CoT 后显著提升 reasoning 任务的判断能力

### 🧠 总结：
> GenRM 是最经典的“生成式评分”方法，而 CoT-GenRM 强调“先解释再判断”的思路，强化逻辑推理偏好。

---

## 4️⃣ **Direct Judgement Preference Optimization**
📅 2024.9 | 📌 方法名：DJPO

### 📌 核心思想：
- 融合多个训练信号：不仅看结果好不好，还引导模型**用不同方式解释**“好在哪、差在哪”

### 📌 三种训练任务：
| 名称 | 说明 |
|------|------|
| **Chain-of-Thought Critique** | 让模型输出带有“逐步推理”的评价逻辑 |
| **Standard Judgement** | 只输出最终判断（Yes/No），不包含解释 |
| **Response Deduction** | 反推原始答案，看模型的评价是否合理 |

### ✅ 方法亮点：
- 多样化监督方式
- 使用强教师模型生成高质量正例，用弱模型生成负例 → 组成偏好对
- 同时使用 DPO loss + SFT loss 进行训练

### 🧠 总结：
> DJPO 把“偏好学习 + 推理 +反推”三种能力结合起来，训练出更稳定、更理解答案质量的评估器。

---

## 5️⃣ **Critique-out-Loud Reward Models**
📅 2024.8 | 📌 方法名：**CLoud**

### 📌 核心思想：
- 奖励模型不再是 “只输出 Yes/No”
- 而是先 **生成一个自然语言的点评评论**
- 然后将 prompt、response、评论 一起输入 → 输出 reward value

### ✅ 方法流程：
1. 输入：prompt + response
2. 模型生成评论文字（如："This answer is correct because..."）
3. 输入 prompt + response + 评论 → 输出 reward score

### ✅ 方法亮点：
- **高可解释性**
- 可解耦：评论模块和打分模块可以分开训练、分析
- 也可以将评论作为训练数据增强的辅助信息

### 🧠 总结：
> CLoud 让模型“边评价边打分”，像一个能说人话的打分器，不仅有输出，还有依据。

---

## ✅ 总结对比表

| 方法/模型名 | 核心机制 | 是否生成语言 | 是否结构改动 | 可解释性 | 是否用 DPO |
|-------------|-----------|----------------|----------------|------------|--------------|
| Con-J | 对比两段生成判断 | ✅ | ❌ | ✅✅✅ | ✅ |
| Generative Verifier | Next-token Yes/No | ❌（只预测 token） | ❌ | ✅ | ❌ or ✅ |
| GenRM | Yes 概率作为 reward | ❌ | ❌ | 中 | ✅ |
| CoT-GenRM | 生成 CoT + 判断 | ✅（生成推理链） | ❌ | ✅✅ | ✅ |
| DJPO | 三种监督方式混合 | ✅ | ❌ | ✅✅ | ✅✅ |
| CLoud | 先生成评论再打分 | ✅✅ | ❌ | ✅✅✅ | 可选 |
