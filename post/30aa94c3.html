<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0"><title>PPO code experiment | detect</title><meta name="author" content="Richard,detect0530@gmail.com"><meta name="copyright" content="Richard"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="PPO code experiment   detect0530@gmail.com  This is a simple experiment to test the PPO algorithm on the OpenAI gym environment. All code resource is from the repository: Link1, which is also inspir">
<meta property="og:type" content="article">
<meta property="og:title" content="PPO code experiment">
<meta property="og:url" content="https://detect42.github.io/post/30aa94c3.html">
<meta property="og:site_name" content="detect">
<meta property="og:description" content="PPO code experiment   detect0530@gmail.com  This is a simple experiment to test the PPO algorithm on the OpenAI gym environment. All code resource is from the repository: Link1, which is also inspir">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://detect42.github.io/img/ff.jpg">
<meta property="article:published_time" content="2024-03-14T15:05:44.000Z">
<meta property="article:modified_time" content="2024-04-08T12:49:52.976Z">
<meta property="article:author" content="Richard">
<meta property="article:tag" content="RL">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://detect42.github.io/img/ff.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://detect42.github.io/post/30aa94c3.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":1000},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: 'days',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  }
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'PPO code experiment',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-04-08 20:49:52'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/1.css"><meta name="generator" content="Hexo 6.3.0"><link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/ff.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">73</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">26</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">26</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/topimg.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="detect"><span class="site-name">detect</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">PPO code experiment</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2024-03-14T15:05:44.000Z" title="Created 2024-03-14 23:05:44">2024-03-14</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2024-04-08T12:49:52.976Z" title="Updated 2024-04-08 20:49:52">2024-04-08</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word count:</span><span class="word-count">2.6k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading time:</span><span>10min</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="PPO code experiment"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><font size=4>
<h1 id="center-ppo-code-experiment-center"><center> PPO code experiment </center></h1>
<h4 id="center-detect0530-gmail-com-center"><center> <a href="mailto:detect0530@gmail.com">detect0530@gmail.com</a> </center></h4>
<p>This is a simple experiment to test the PPO algorithm on the OpenAI gym environment.</p>
<p>All code resource is from the repository: <a target="_blank" rel="noopener" href="https://github.com/Lizhi-sjtu/DRL-code-pytorch/tree/main">Link1</a>, which is also inspired by a blog paper from <a target="_blank" rel="noopener" href="https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/">37 PPO Tricks</a>.</p>
<p><strong>Here, I’d like to split the code frame, and note what I have learned from the code.</strong></p>
<h2 id="code-frame">Code Frame</h2>
<ul>
<li>
<p>main</p>
<ul>
<li>argparse, set the hyperparameter</li>
<li>tensorboard, log the training process</li>
<li>evaluate policy, test the policy</li>
<li>train policy, train the policy</li>
</ul>
</li>
<li>
<p>ppo_agent</p>
<ul>
<li>Actor Module
<ul>
<li>its distrubution</li>
<li>network structure</li>
</ul>
</li>
<li>Critic Module
<ul>
<li>network structure</li>
</ul>
</li>
<li>PPO Agent
<ul>
<li>evaluate, return the deterministic action(mean)</li>
<li>choose_action, return the <strong>sampled action</strong> and its log probability</li>
<li>update, update the policy and value network when the replay buffer is full. Note that it can be updated multiple times in it since we use the IS ratio trick.</li>
</ul>
</li>
</ul>
</li>
<li>
<p>ReplayBuffer</p>
<ul>
<li>init, use np dtype to store</li>
<li>store, store the transition</li>
<li>numpy_to_tensorple, Once the buffer is full, “agent.update” will turn all the data in buffer to tensor, and then update the policy and value network.</li>
</ul>
</li>
<li>
<p>Normalization</p>
<ul>
<li>RunningMeanStd, calculate the mean and std of the input data with dynamic method.</li>
<li>Normalization, a class which can normalize data to zero mean and unit variance.</li>
<li>RewardScaling, a trick to scale the reward to a proper range.(just divide the reward by std)</li>
</ul>
</li>
</ul>
<hr>
<h2 id="trick-explanation">Trick explanation</h2>
<p>In the following, I’ll explain 12 tricks in the PPO algorithm. Some of them indeed works, but some are obscure to me, which means sometimes it works but sometimes it even reduces the performance.</p>
<hr>
<h3 id="1-use-gae-to-estimate-the-advantage">1. use GAE to estimate the advantage</h3>
<p><img src="ppo_code/image.png" alt="alt text"></p>
<p>make the advantage estimation more stable.</p>
<p>for more details, you can view my another blog post <a href="">RL_toolbox</a></p>
<hr>
<h3 id="2-use-the-clipped-is-ratio-to-update-the-policy">2. use the clipped IS ratio to update the policy</h3>
<p><img src="ppo_code/image-1.png" alt="alt text"></p>
<p>make the update more stable and avoid the large variance.</p>
<hr>
<h3 id="3-advantage-normalization">3. Advantage Normalization</h3>
<p>After we calculate all the advantage in a batch via GAE, we normalize the advantage to zero mean and unit variance.</p>
<p><strong>it indeed play an important role in the training process. Training almost cannot be completed without this trick!</strong></p>
<hr>
<h3 id="4-state-normalization">4. State Normalization</h3>
<p>The core of state normalization is to maintain a running mean and std of the state, and then normalize the state to zero mean and unit variance. Pay attention to that we need to dynamically update the mean and std of all the states.</p>
<p>After state normalization, the policy network can be trained more efficiently with the normalized state input.</p>
<p><img src="ppo_code/image-2.png" alt="alt text"><br>
<img src="ppo_code/image-3.png" alt="alt text"></p>
<hr>
<h3 id="5-reward-scaling">5. Reward Scaling</h3>
<ul>
<li>by chatgpt:</li>
</ul>
<p>在论文《PPO-Implementation matters in deep policy gradients: A case study on PPO and TRPO》中提出的reward scaling技术，通过动态计算回报的滚动折现和的标准差，并将当前奖励除以这个标准差进行缩放，具有以下优点：</p>
<ol>
<li>
<p><strong>改善学习稳定性</strong>：通过将奖励标准化，reward scaling有助于维持学习过程中的数值稳定性。这是因为缩放后的奖励通常会落在一个相对较小和更统一的数值范围内，减少了学习算法在面对极端或不同量级的奖励时的不稳定性。</p>
</li>
<li>
<p><strong>加快收敛速度</strong>：标准化处理能够使得奖励信号更加一致，从而加速学习过程的收敛。缩放奖励意味着梯度更新可以在一个更加一致的尺度上进行，避免了因为奖励尺度问题导致的学习速度变慢。</p>
</li>
<li>
<p><strong>自适应调整</strong>：与静态的奖励缩放方法不同，reward scaling根据实时计算的标准差动态调整奖励的缩放程度。这意味着它能够自适应不同环境和任务中奖励分布的变化，无需手动调整缩放因子。</p>
</li>
<li>
<p><strong>减少训练过程中的方差</strong>：通过缩放奖励，这种方法有助于减少训练过程中策略梯度的方差。较低的方差可以提高策略更新的质量，从而提高整个训练过程的效率和可靠性。</p>
</li>
<li>
<p><strong>易于实现和集成</strong>：作为一种预处理步骤，reward scaling相对简单，容易在现有的强化学习框架和算法中实现和集成。这使得它成为一种低成本且有效的方法，用于提升深度策略梯度方法的性能。</p>
</li>
</ol>
<p>总的来说，reward scaling提供了一种有效的技术手段，通过动态调整奖励缩放程度，改善强化学习模型的训练稳定性、加速模型收敛，并提高训练过程的自适应性和效率。</p>
<p><img src="ppo_code/image-4.png" alt="alt text"></p>
<hr>
<h3 id="6-policy-entropy">6. Policy Entropy</h3>
<p>we use entropy to represent the uncertainty of the policy. The entropy is used to encourage the policy to explore more in the environment.</p>
<p>In our code, when we get the distribution of the action (action_dim) from actor network, we can calculate the entropy of the action distribution by the following code:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">dist_entropy = dist_now.entropy().<span class="built_in">sum</span>(<span class="number">1</span>,keepdim=<span class="literal">True</span>)</span><br><span class="line">actor_loss = -torch.<span class="built_in">min</span>(surr1,surr2) - self.entropy_coef * dist_entropy</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="7-learning-rate-decay">7. Learning Rate Decay</h3>
<p>Learning rate decay can enhance the stability of the later stages of training to a certain extent. We here utilize the learning rate linear decay method.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">lr_decay</span>(<span class="params">self, total_steps</span>):</span><br><span class="line">    lr_a_now = self.lr_a * (<span class="number">1</span> - total_steps / self.max_train_steps)</span><br><span class="line">    lr_c_now = self.lr_c * (<span class="number">1</span> - total_steps / self.max_train_steps)</span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> self.optimizer_actor.param_groups:</span><br><span class="line">        p[<span class="string">&#x27;lr&#x27;</span>] = lr_a_now</span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> self.optimizer_critic.param_groups:</span><br><span class="line">        p[<span class="string">&#x27;lr&#x27;</span>] = lr_c_now</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="8-gradient-clip">8. Gradient Clip</h3>
<p>To prevent gradient explosion, we use the gradient clip trick.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Update actor</span></span><br><span class="line">self.optimizer_actor.zero_grad()</span><br><span class="line">actor_loss.mean().backward()</span><br><span class="line"><span class="keyword">if</span> self.use_grad_clip: <span class="comment"># Trick 7: Gradient clip</span></span><br><span class="line">    torch.nn.utils.clip_grad_norm_(self.actor.parameters(), <span class="number">0.5</span>)</span><br><span class="line">self.optimizer_actor.step()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Update critic</span></span><br><span class="line">self.optimizer_critic.zero_grad()</span><br><span class="line">critic_loss.backward()</span><br><span class="line"><span class="keyword">if</span> self.use_grad_clip: <span class="comment"># Trick 7: Gradient clip</span></span><br><span class="line">    torch.nn.utils.clip_grad_norm_(self.critic.parameters(), <span class="number">0.5</span>)</span><br><span class="line">self.optimizer_critic.step()</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="9-orthogonal-initialization">9. Orthogonal Initialization</h3>
<p>To prevent gradient vanishing or explosion in the beginning of training, we use the orthogonal initialization trick.</p>
<p>I still query GPT for details of this trick.</p>
<p>正交初始化（Orthogonal Initialization）是一种常用于深度学习模型中的参数初始化方法，特别是在训练深层神经网络时。这种方法有几个显著的优点：</p>
<p><strong>优点有哪些?</strong></p>
<h4 id="1-缓解梯度消失或爆炸问题">1. 缓解梯度消失或爆炸问题</h4>
<p>在深度神经网络中，梯度消失或爆炸是训练过程中常见的问题。正交初始化通过保持权重矩阵的正交性，有助于维持激活函数的输入在一个合理的范围内，这样可以有效地减少梯度消失或爆炸的问题，使得梯度更稳定地在网络中传播。</p>
<h4 id="2-促进网络训练的稳定性">2. 促进网络训练的稳定性</h4>
<p>正交初始化能够促进网络训练过程的稳定性。由于初始化的权重矩阵是正交的，这有助于每层的输入保持独立性，减少了层间的相互干扰。这种独立性有助于稳定训练过程，特别是在训练深层网络时更是如此。</p>
<h4 id="3-加速收敛">3. 加速收敛</h4>
<p>通过减少梯度消失和爆炸的问题，以及提高网络训练的稳定性，正交初始化有助于加速模型的收敛速度。网络参数的优良初始值可以让模型更快地达到较低的训练误差和泛化误差。</p>
<h4 id="4-改善深层网络的性能">4. 改善深层网络的性能</h4>
<p>对于深层网络，正交初始化特别有利于提高其性能。深层网络因为层数多，梯度在反向传播时很容易受到消失或爆炸的影响，正交初始化通过维持权重的正交性，有助于梯度更平稳地通过网络的每一层，从而有助于提高深层网络的学习能力和最终的模型性能。</p>
<h4 id="5-适用于多种网络结构">5. 适用于多种网络结构</h4>
<p>正交初始化不仅适用于全连接网络，也适用于卷积网络和循环神经网络等多种网络结构。特别是在循环神经网络（RNN）中，正交初始化可以有效地缓解梯度消失的问题，提高长期依赖的学习能力。</p>
<p>总之，正交初始化通过提供一种稳定且有效的方式来初始化网络权重，有助于改善训练过程的稳定性、加速模型的收敛，并提高深层网络的性能。</p>
<p><strong>为什么正交初始化可以缓解梯度消失或爆炸问题?</strong></p>
<p>正交初始化能够缓解梯度消失或爆炸问题的原因主要与深度神经网络中权重矩阵的乘积以及梯度传播有关。在深度神经网络的训练过程中，输入信号和梯度需要通过多层的权重矩阵传播。如果这些权重矩阵的初始化不当，它们的乘积很容易导致数值过大或过小，进而引起梯度爆炸或梯度消失问题。正交初始化通过确保权重矩阵的正交性，有助于控制这一问题：</p>
<h4 id="1-保持信号范围">1. 保持信号范围</h4>
<p>正交矩阵有一个重要的性质：它们的列（或行）向量互相正交且单位化（即长度为1）。这意味着，当数据通过一个用正交矩阵初始化的网络层时，输入信号的范数（或长度）被保持不变。因此，在理想情况下，正交初始化有助于在深度网络的各层之间保持激活值和梯度的规模，减少梯度消失或爆炸的风险。</p>
<h4 id="2-促进梯度的稳定传播">2. 促进梯度的稳定传播</h4>
<p>在深度网络中，梯度通过反向传播算法从输出层传播回输入层。如果权重矩阵非正交，梯度在传播过程中可能会迅速增长或减少，导致爆炸或消失。由于正交矩阵保持了向量长度，这有助于梯度稳定地传播，即使网络很深。</p>
<h4 id="3-减少内部协变量偏移">3. 减少内部协变量偏移</h4>
<p>正交初始化还有助于减少训练过程中的内部协变量偏移，即网络中不同层激活分布的变化。由于正交初始化保持了输入信号的规模，它有助于使每一层的输入分布保持相对稳定，从而减轻训练初期的学习负担。</p>
<h4 id="4-提高学习效率">4. 提高学习效率</h4>
<p>由于梯度的稳定传播，网络可以更有效地学习，避免了因梯度消失导致的学习停滞，或因梯度爆炸导致的训练不稳定。这种效率的提高有助于加速收敛，提高模型性能。</p>
<p>总结而言，正交初始化之所以能够缓解梯度消失或爆炸问题，主要是因为它保持了深度网络中信号和梯度的规模，确保了训练过程中梯度的稳定传播，这对于深度网络的训练至关重要。</p>
<hr>
<h3 id="10-adam-optimizer-epsilon-parameter">10. Adam Optimizer Epsilon Parameter</h3>
<p>we turn it from eps = 1e-8 to eps = 1e-5, which can make the training more stable.</p>
<hr>
<h3 id="11-tanh-activate-function">11. Tanh Activate Function</h3>
<p>Just use it. I don’t know exactly why it works.</p>
<hr>
<h3 id="12-gaussian-distribution-and-beta-distribution">12. Gaussian Distribution and Beta Distribution</h3>
<p>In fact, we use Gaussian distribution to output the action most of time. But we need to clip the action to a proper range as Gaussian distribution is unbounded, which induce a negative effect on the performance.</p>
<p>We try to use Beta distribution to output action in a range of [0,1]. Then we can map [0.1] to any action range.</p>
<p><img src="ppo_code/image-5.png" alt="alt text"></p>
<hr>
<h2 id="code-details">Code details</h2>
<p>If you want to know more about the code details, maybe you can read my another blog post <a href="">RL_Code_Details</a> which analyze some code details. PPO code is included of course.</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="https://detect42.github.io">Richard</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://detect42.github.io/post/30aa94c3.html">https://detect42.github.io/post/30aa94c3.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/RL/">RL</a></div><div class="post_share"><div class="social-share" data-image="/img/ff.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/post/22c2c40d.html" title="Python Grammar"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">Python Grammar</div></div></a></div><div class="next-post pull-right"><a href="/post/96345fc2.html" title="RL_toolbox"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">RL_toolbox</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/post/d5d94bd9.html" title="Deep Generative Model"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-10-09</div><div class="title">Deep Generative Model</div></div></a></div><div><a href="/post/61815766.html" title="MOPO"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-10-09</div><div class="title">MOPO</div></div></a></div><div><a href="/post/4668f2fa.html" title="COMBO"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-10-08</div><div class="title">COMBO</div></div></a></div><div><a href="/post/56cbbc62.html" title="TRPO"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-17</div><div class="title">TRPO</div></div></a></div><div><a href="/post/970c2795.html" title="SAC"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-25</div><div class="title">SAC</div></div></a></div><div><a href="/post/61815765.html" title="Proximal Policy Optimization(PPO)"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-14</div><div class="title">Proximal Policy Optimization(PPO)</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#center-ppo-code-experiment-center"><span class="toc-text"> PPO code experiment </span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#center-detect0530-gmail-com-center"><span class="toc-text"> detect0530@gmail.com </span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#code-frame"><span class="toc-text">Code Frame</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#trick-explanation"><span class="toc-text">Trick explanation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-use-gae-to-estimate-the-advantage"><span class="toc-text">1. use GAE to estimate the advantage</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-use-the-clipped-is-ratio-to-update-the-policy"><span class="toc-text">2. use the clipped IS ratio to update the policy</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-advantage-normalization"><span class="toc-text">3. Advantage Normalization</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-state-normalization"><span class="toc-text">4. State Normalization</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-reward-scaling"><span class="toc-text">5. Reward Scaling</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-policy-entropy"><span class="toc-text">6. Policy Entropy</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-learning-rate-decay"><span class="toc-text">7. Learning Rate Decay</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-gradient-clip"><span class="toc-text">8. Gradient Clip</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-orthogonal-initialization"><span class="toc-text">9. Orthogonal Initialization</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E7%BC%93%E8%A7%A3%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E6%88%96%E7%88%86%E7%82%B8%E9%97%AE%E9%A2%98"><span class="toc-text">1. 缓解梯度消失或爆炸问题</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E4%BF%83%E8%BF%9B%E7%BD%91%E7%BB%9C%E8%AE%AD%E7%BB%83%E7%9A%84%E7%A8%B3%E5%AE%9A%E6%80%A7"><span class="toc-text">2. 促进网络训练的稳定性</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E5%8A%A0%E9%80%9F%E6%94%B6%E6%95%9B"><span class="toc-text">3. 加速收敛</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-%E6%94%B9%E5%96%84%E6%B7%B1%E5%B1%82%E7%BD%91%E7%BB%9C%E7%9A%84%E6%80%A7%E8%83%BD"><span class="toc-text">4. 改善深层网络的性能</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-%E9%80%82%E7%94%A8%E4%BA%8E%E5%A4%9A%E7%A7%8D%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84"><span class="toc-text">5. 适用于多种网络结构</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E4%BF%9D%E6%8C%81%E4%BF%A1%E5%8F%B7%E8%8C%83%E5%9B%B4"><span class="toc-text">1. 保持信号范围</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E4%BF%83%E8%BF%9B%E6%A2%AF%E5%BA%A6%E7%9A%84%E7%A8%B3%E5%AE%9A%E4%BC%A0%E6%92%AD"><span class="toc-text">2. 促进梯度的稳定传播</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E5%87%8F%E5%B0%91%E5%86%85%E9%83%A8%E5%8D%8F%E5%8F%98%E9%87%8F%E5%81%8F%E7%A7%BB"><span class="toc-text">3. 减少内部协变量偏移</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-%E6%8F%90%E9%AB%98%E5%AD%A6%E4%B9%A0%E6%95%88%E7%8E%87"><span class="toc-text">4. 提高学习效率</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-adam-optimizer-epsilon-parameter"><span class="toc-text">10. Adam Optimizer Epsilon Parameter</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-tanh-activate-function"><span class="toc-text">11. Tanh Activate Function</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-gaussian-distribution-and-beta-distribution"><span class="toc-text">12. Gaussian Distribution and Beta Distribution</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#code-details"><span class="toc-text">Code details</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image: url('/img/topimg.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By Richard</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></body></html>