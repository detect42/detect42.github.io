<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0"><title>GPT问答_NN数据拟合源码拆解 | detect</title><meta name="author" content="Richard,detect0530@gmail.com"><meta name="copyright" content="Richard"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Sample code for PyTorch 12345678def same_seed(seed):    &amp;#x27;&amp;#x27;&amp;#x27;Fixes random number generator seeds for reproducibility.&amp;#x27;&amp;#x27;&amp;#x27;    torch.backends.cudnn.deterministic &#x3D; True    tor">
<meta property="og:type" content="article">
<meta property="og:title" content="GPT问答_NN数据拟合源码拆解">
<meta property="og:url" content="https://detect42.github.io/post/a43615c7.html">
<meta property="og:site_name" content="detect">
<meta property="og:description" content="Sample code for PyTorch 12345678def same_seed(seed):    &amp;#x27;&amp;#x27;&amp;#x27;Fixes random number generator seeds for reproducibility.&amp;#x27;&amp;#x27;&amp;#x27;    torch.backends.cudnn.deterministic &#x3D; True    tor">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://detect42.github.io/img/ff.jpg">
<meta property="article:published_time" content="2023-10-19T03:26:03.000Z">
<meta property="article:modified_time" content="2024-03-07T13:10:03.875Z">
<meta property="article:author" content="Richard">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="GPT">
<meta property="article:tag" content="神经网络">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://detect42.github.io/img/ff.jpg"><link rel="shortcut icon" href="/img/ff.jpg"><link rel="canonical" href="https://detect42.github.io/post/a43615c7.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":1000},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: 'days',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  }
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'GPT问答_NN数据拟合源码拆解',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-03-07 21:10:03'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/1.css"><meta name="generator" content="Hexo 6.3.0"><link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/ff.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">70</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">26</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">26</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/topimg.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="detect"><span class="site-name">detect</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">GPT问答_NN数据拟合源码拆解</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2023-10-19T03:26:03.000Z" title="Created 2023-10-19 11:26:03">2023-10-19</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2024-03-07T13:10:03.875Z" title="Updated 2024-03-07 21:10:03">2024-03-07</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/DL/">DL</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/DL/code/">code</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word count:</span><span class="word-count">12.3k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading time:</span><span>42min</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="GPT问答_NN数据拟合源码拆解"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="sample-code-for-pytorch">Sample code for PyTorch</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">same_seed</span>(<span class="params">seed</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;Fixes random number generator seeds for reproducibility.&#x27;&#x27;&#x27;</span></span><br><span class="line">    torch.backends.cudnn.deterministic = <span class="literal">True</span></span><br><span class="line">    torch.backends.cudnn.benchmark = <span class="literal">False</span></span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    torch.manual_seed(seed)</span><br><span class="line">    <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">        torch.cuda.manual_seed_all(seed)</span><br></pre></td></tr></table></figure>
<p>这是一个函数，看起来用于设置随机数生成器的种子以实现可重复性。以下是每一句的解释：</p>
<ol>
<li>
<p><code>def same_seed(seed):</code></p>
<ul>
<li>这是一个Python函数的定义，名为<code>same_seed</code>，它接受一个参数<code>seed</code>。</li>
</ul>
</li>
<li>
<p><code>'''Fixes random number generator seeds for reproducibility.'''</code></p>
<ul>
<li>这是函数的文档字符串，提供了对函数目的的简要描述。</li>
</ul>
</li>
<li>
<p><code>torch.backends.cudnn.deterministic = True</code></p>
<ul>
<li>这一行设置了PyTorch的CuDNN后端以确定性模式。这意味着CuDNN将使用固定的算法，以确保每次运行代码时生成的结果都相同，以实现可重复性。</li>
</ul>
</li>
<li>
<p><code>torch.backends.cudnn.benchmark = False</code></p>
<ul>
<li>这一行禁用了CuDNN的性能基准。通常，CuDNN会根据硬件性能自动选择最佳算法，但禁用它可以确保每次都使用相同的算法，以实现可重复性。</li>
</ul>
</li>
<li>
<p><code>np.random.seed(seed)</code></p>
<ul>
<li>这一行设置了NumPy库中随机数生成器的种子，确保NumPy的随机操作也是可重复的。</li>
</ul>
</li>
<li>
<p><code>torch.manual_seed(seed)</code></p>
<ul>
<li>这一行设置了PyTorch的随机数生成器的种子，以确保PyTorch中的随机操作也是可重复的。</li>
</ul>
</li>
<li>
<p><code>if torch.cuda.is_available():</code></p>
<ul>
<li>这一行检查是否有可用的CUDA（GPU）设备。如果系统支持GPU，并且GPU可用，它将执行以下代码块。</li>
</ul>
</li>
<li>
<p><code>torch.cuda.manual_seed_all(seed)</code></p>
<ul>
<li>这一行设置所有可用的CUDA设备上的随机数生成器的种子，以确保在使用GPU时也能实现可重复性。</li>
</ul>
</li>
</ol>
<p>这个函数的作用是在深度学习任务中设置各种随机数生成器的种子，以确保在每次运行相同的代码时，结果都是可重复的，这对于实验和调试非常有用。</p>
<hr>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_valid_split</span>(<span class="params">data_set, valid_ratio, seed</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;Split provided training data into training set and validation set&#x27;&#x27;&#x27;</span></span><br><span class="line">    valid_set_size = <span class="built_in">int</span>(valid_ratio * <span class="built_in">len</span>(data_set))</span><br><span class="line">    train_set_size = <span class="built_in">len</span>(data_set) - valid_set_size</span><br><span class="line">    train_set, valid_set = random_split(data_set, [train_set_size, valid_set_size], generator=torch.Generator().manual_seed(seed))</span><br><span class="line">    <span class="keyword">return</span> np.array(train_set), np.array(valid_set)</span><br></pre></td></tr></table></figure>
<p>这是一个函数，用于将提供的训练数据集分割成训练集和验证集。以下是每一句的解释：</p>
<ol>
<li>
<p><code>def train_valid_split(data_set, valid_ratio, seed):</code></p>
<ul>
<li>这是一个Python函数的定义，名为<code>train_valid_split</code>，它接受三个参数：<code>data_set</code>（数据集），<code>valid_ratio</code>（验证集比例），和<code>seed</code>（随机数种子）。</li>
</ul>
</li>
<li>
<p><code>'''Split provided training data into training set and validation set'''</code></p>
<ul>
<li>这是函数的文档字符串，提供了对函数目的的简要描述。</li>
</ul>
</li>
<li>
<p><code>valid_set_size = int(valid_ratio * len(data_set))</code></p>
<ul>
<li>这一行计算了验证集的大小。<code>valid_ratio</code> 是一个介于0和1之间的比例，表示验证集在总数据集中的比例。<code>len(data_set)</code> 给出了总数据集的大小。</li>
</ul>
</li>
<li>
<p><code>train_set_size = len(data_set) - valid_set_size</code></p>
<ul>
<li>这一行计算了训练集的大小，即总数据集大小减去验证集的大小。</li>
</ul>
</li>
<li>
<p><code>train_set, valid_set = random_split(data_set, [train_set_size, valid_set_size], generator=torch.Generator().manual_seed(seed))</code></p>
<ul>
<li>这一行使用<code>random_split</code>函数将数据集分割成训练集和验证集。<code>random_split</code>函数接受三个参数：数据集本身 (<code>data_set</code>)，一个包含两个整数的列表，分别表示训练集和验证集的大小 (<code>[train_set_size, valid_set_size]</code>)，以及一个随机数生成器 (<code>torch.Generator().manual_seed(seed)</code>)，以确保拆分是可重复的。函数返回训练集 (<code>train_set</code>) 和验证集 (<code>valid_set</code>)。</li>
</ul>
</li>
<li>
<p><code>return np.array(train_set), np.array(valid_set)</code></p>
<ul>
<li>最后，函数返回训练集和验证集，这些集合被转换为NumPy数组（<code>np.array</code>），以便后续使用。</li>
</ul>
</li>
</ol>
<p>这个函数的作用是将给定的训练数据集按照指定的验证集比例和种子随机分成训练集和验证集，以用于深度学习模型的训练和验证。</p>
<h3 id="关于随机数生成器">关于随机数生成器</h3>
<p>您可以使用其他随机数生成器替代<code>torch.Generator().manual_seed(seed)</code>，但具体要使用哪个生成器取决于您的需求和代码中使用的深度学习框架。</p>
<p>在PyTorch中，<code>torch.Generator().manual_seed(seed)</code>是一种设置种子以确保随机性可重复的方式。如果您想使用其他生成器，通常可以考虑使用Python内置的<code>random</code>模块或NumPy的随机模块，例如<code>numpy.random.RandomState</code>。以下是一些示例：</p>
<ol>
<li>
<p>使用Python内置的<code>random</code>模块：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">random.seed(seed)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>使用NumPy的随机模块：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">rng = np.random.RandomState(seed)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>这些生成器也可以用于设置随机种子以确保可重复性。但请注意，如果您使用的是PyTorch或其他深度学习框架的功能，通常最好使用该框架提供的生成器，以确保与框架的其他随机操作相协调。在这种情况下，使用<code>torch.Generator().manual_seed(seed)</code>可能是更好的选择。</p>
<hr>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">test_loader, model, device</span>):</span><br><span class="line">    model.<span class="built_in">eval</span>() <span class="comment"># Set your model to evaluation mode.</span></span><br><span class="line">    preds = []</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> tqdm(test_loader):</span><br><span class="line">        x = x.to(device)</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            pred = model(x)</span><br><span class="line">            preds.append(pred.detach().cpu())</span><br><span class="line">    preds = torch.cat(preds, dim=<span class="number">0</span>).numpy()</span><br><span class="line">    <span class="keyword">return</span> preds</span><br></pre></td></tr></table></figure>
<p>这是一个Python函数，用于使用PyTorch模型进行推断（预测）。下面是它的逐句解释：</p>
<ol>
<li>
<p><code>def predict(test_loader, model, device):</code></p>
<ul>
<li>这是一个Python函数的定义，名为<code>predict</code>，它接受三个参数：<code>test_loader</code>，<code>model</code>和<code>device</code>。</li>
</ul>
</li>
<li>
<p><code>model.eval()</code></p>
<ul>
<li><code>model.eval()</code> 是PyTorch中的方法，将神经网络模型切换到评估（evaluation）模式。在评估模式下，模型不会进行梯度计算，通常用于推断（inference）阶段，以加快计算速度并节省内存。</li>
</ul>
</li>
<li>
<p><code>preds = []</code></p>
<ul>
<li>创建一个空列表 <code>preds</code> 以存储模型的预测结果。</li>
</ul>
</li>
<li>
<p><code>for x in tqdm(test_loader):</code></p>
<ul>
<li>这是一个循环，遍历<code>test_loader</code>，<code>test_loader</code>通常是一个数据加载器（data loader），用于提供测试数据。<code>tqdm</code>用于在循环中创建一个进度条，以便跟踪处理进度。</li>
</ul>
</li>
<li>
<p><code>x = x.to(device)</code></p>
<ul>
<li>将输入数据 <code>x</code> 移动到指定的计算设备（通常是GPU或CPU），由<code>device</code>参数指定。</li>
</ul>
</li>
<li>
<p><code>with torch.no_grad():</code></p>
<ul>
<li><code>with torch.no_grad():</code> 是一个上下文管理器，用于告诉PyTorch不要跟踪计算梯度，因为在评估模式下，不需要梯度信息。</li>
</ul>
</li>
<li>
<p><code>pred = model(x)</code></p>
<ul>
<li>使用模型 <code>model</code> 对输入数据 <code>x</code> 进行预测，生成预测结果 <code>pred</code>。</li>
</ul>
</li>
<li>
<p><code>preds.append(pred.detach().cpu())</code></p>
<ul>
<li>将预测结果 <code>pred</code> 从计算设备上移除（<code>.detach()</code>）并移动到CPU上（<code>.cpu()</code>），然后将它添加到 <code>preds</code> 列表中。</li>
</ul>
</li>
<li>
<p><code>preds = torch.cat(preds, dim=0).numpy()</code></p>
<ul>
<li>将列表中的所有预测结果 <code>preds</code> 连接成一个张量，连接维度为 <code>dim=0</code>，然后将结果转换为NumPy数组，最终返回这个NumPy数组。</li>
</ul>
</li>
<li>
<p><code>return preds</code></p>
<ul>
<li>返回包含所有测试样本预测结果的NumPy数组。</li>
</ul>
</li>
</ol>
<p>这个函数的作用是使用给定的PyTorch模型对测试数据进行推断，返回一个包含所有预测结果的NumPy数组。在这个过程中，确保模型处于评估模式，不进行梯度计算，提高推断效率。</p>
<hr>
<p><code>with torch.no_grad():</code> 上下文管理器的作用是告诉PyTorch在这个上下文中不要跟踪计算梯度。这在推断（inference）阶段非常有用，具体原因如下：</p>
<ol>
<li>
<p><strong>梯度计算开销：</strong> 在训练模型时，我们需要计算每个操作的梯度，以便进行反向传播和参数更新。然而，在推断阶段，我们通常不需要这些梯度信息，因为我们只是用模型进行前向传播，计算预测结果。</p>
</li>
<li>
<p><strong>内存优化：</strong> 禁用梯度跟踪可以显著减少内存占用。在训练期间，PyTorch会为每个操作保留梯度信息，这可能占用大量内存。在推断期间，我们可以通过使用<code>with torch.no_grad():</code> 来减少内存使用，特别是当处理大量数据时。</p>
</li>
<li>
<p><strong>加速计算：</strong> 由于不进行梯度计算，禁用梯度跟踪还可以提高计算速度。这对于实时应用或需要高性能的情况非常有帮助。</p>
</li>
</ol>
<p>因此，当你在推断模型时，使用<code>with torch.no_grad():</code> 可以提高效率，减少内存占用，并确保不会因不必要的梯度计算而导致性能下降。</p>
<hr>
<p><code>preds.append(pred.detach().cpu())</code> 的主要原因是将预测结果从GPU（或其他计算设备）移动到CPU是为了使其更易于处理和使用。这样做通常是有几个原因的：</p>
<ol>
<li>
<p><strong>兼容性：</strong> 在PyTorch中，如果你的模型在GPU上计算，那么GPU上的张量与CPU上的张量具有不同的类型。在某些情况下，你可能希望将结果移回CPU，以便与其他CPU上的数据进行处理和组合。</p>
</li>
<li>
<p><strong>NumPy互操作：</strong> 很多时候，你可能需要将PyTorch的张量转换为NumPy数组以便进行进一步的数据处理或可视化。通常，NumPy更容易与CPU上的数据交互，所以将数据移回CPU使得这一过程更简单。</p>
</li>
<li>
<p><strong>内存管理：</strong> GPU上的内存通常有限，将不再需要的数据移回CPU可以释放GPU内存，以便在后续计算中使用。这对于大型模型或大规模数据处理非常重要。</p>
</li>
</ol>
<p>总之，将预测结果从GPU移回CPU通常是为了提高兼容性、便于后续处理，以及有效地管理内存。但需要根据具体的应用场景和需求来决定是否需要这个步骤。如果你只在GPU上进行后续计算，并且不需要与CPU上的数据交互，那么可以跳过这一步。</p>
<hr>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">COVID19Dataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    x: Features.</span></span><br><span class="line"><span class="string">    y: Targets, if none, do prediction.</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, x, y=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="keyword">if</span> y <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            self.y = y</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.y = torch.FloatTensor(y)</span><br><span class="line">        self.x = torch.FloatTensor(x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="keyword">if</span> self.y <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> self.x[idx]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> self.x[idx], self.y[idx]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.x)</span><br></pre></td></tr></table></figure>
<p>这是一个自定义的PyTorch数据集类 <code>COVID19Dataset</code>，用于加载和处理COVID-19数据集。以下是该类的逐句解释：</p>
<ol>
<li>
<p><code>class COVID19Dataset(Dataset):</code></p>
<ul>
<li>这是一个类定义，创建了一个名为 <code>COVID19Dataset</code> 的PyTorch数据集类，它继承自 <code>Dataset</code> 类。</li>
</ul>
</li>
<li>
<p><code>def __init__(self, x, y=None):</code></p>
<ul>
<li>这是类的构造函数，它接受两个参数：<code>x</code> 和可选的 <code>y</code>，其中 <code>x</code> 是特征数据，<code>y</code> 是目标数据（如果提供了）。如果没有提供目标数据，你可以在构造数据集对象时将 <code>y</code> 参数设置为 <code>None</code>。</li>
</ul>
</li>
<li>
<p><code>if y is None: self.y = y</code></p>
<ul>
<li>这里检查是否提供了目标数据 <code>y</code>。如果没有提供，<code>self.y</code> 被设置为 <code>None</code>，表示这是一个用于预测的数据集，而不是用于监督学习的数据集。</li>
</ul>
</li>
<li>
<p><code>self.y = torch.FloatTensor(y)</code></p>
<ul>
<li>如果提供了目标数据 <code>y</code>，则将其转换为PyTorch张量并将其存储在 <code>self.y</code> 中。这假设目标数据是浮点数类型。</li>
</ul>
</li>
<li>
<p><code>self.x = torch.FloatTensor(x)</code></p>
<ul>
<li>将特征数据 <code>x</code> 转换为PyTorch张量并将其存储在 <code>self.x</code> 中。同样，这假设特征数据是浮点数类型。</li>
</ul>
</li>
<li>
<p><code>def __getitem__(self, idx):</code></p>
<ul>
<li>这是一个特殊方法，用于定义如何获取数据集中的单个样本。根据是否提供了目标数据，它返回不同的内容。如果没有提供目标数据，只返回特征数据；否则，返回特征数据和目标数据。</li>
</ul>
</li>
<li>
<p><code>def __len__(self):</code></p>
<ul>
<li>这是另一个特殊方法，用于返回数据集的长度，即数据集中包含多少个样本。它返回特征数据 <code>self.x</code> 的长度。</li>
</ul>
</li>
</ol>
<p>这个自定义数据集类的主要作用是为PyTorch提供一种有效的方式来加载和处理COVID-19数据，适用于监督学习和预测任务，具体取决于是否提供了目标数据。这样的数据集类通常在训练和测试神经网络时非常有用。</p>
<hr>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">My_Model</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>(My_Model, self).__init__()</span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> modify model&#x27;s structure, be aware of dimensions.</span></span><br><span class="line">        self.layers = nn.Sequential(</span><br><span class="line">            nn.Linear(input_dim, <span class="number">16</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">16</span>, <span class="number">8</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">8</span>, <span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.layers(x)</span><br><span class="line">        x = x.squeeze(<span class="number">1</span>) <span class="comment"># (B, 1) -&gt; (B)</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>这是一个自定义的PyTorch神经网络模型类 <code>My_Model</code>，以下是该类的逐句解释：</p>
<ol>
<li>
<p><code>class My_Model(nn.Module):</code></p>
<ul>
<li>这是一个类定义，创建了一个名为 <code>My_Model</code> 的PyTorch神经网络模型类，它继承自 <code>nn.Module</code>。</li>
</ul>
</li>
<li>
<p><code>def __init__(self, input_dim):</code></p>
<ul>
<li>这是类的构造函数，它接受一个参数 <code>input_dim</code>，表示输入数据的维度。</li>
</ul>
</li>
<li>
<p><code>super(My_Model, self).__init__()</code></p>
<ul>
<li>在构造函数中，首先调用父类 <code>nn.Module</code> 的构造函数以初始化模型。</li>
</ul>
</li>
<li>
<p><code>self.layers = nn.Sequential(...)</code></p>
<ul>
<li>这里定义了神经网络的结构。<code>self.layers</code> 是一个由多个层组成的序列（Sequential）模块。模型的结构包括三个线性层（全连接层）和两激活函数层。具体结构如下：
<ul>
<li>输入层：接受大小为 <code>input_dim</code> 的输入。</li>
<li>第一个隐藏层：包含 16 个神经元，并使用 ReLU（Rectified Linear Unit）激活函数。</li>
<li>第二个隐藏层：包含 8 个神经元，并使用 ReLU 激活函数。</li>
<li>输出层：包含 1 个神经元，用于回归任务。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><code>def forward(self, x):</code></p>
<ul>
<li>这是模型的前向传播方法，用于定义数据在模型中的传递过程。</li>
<li><code>x = self.layers(x)</code> 执行了输入数据 <code>x</code> 经过定义的层序列 <code>self.layers</code> 的前向传播。</li>
<li><code>x = x.squeeze(1)</code> 将输出张量中的维度 1 压缩，从 (B, 1) 到 (B)，其中 B 表示批量大小（Batch Size）。</li>
<li>最终返回处理后的张量 <code>x</code> 作为模型的输出。</li>
</ul>
</li>
</ol>
<hr>
<p>这是一个很好的问题，涉及到Python面向对象编程和继承中的特定语法。在这里，<code>My_Model</code>是一个类，它继承自<code>nn.Module</code>，这是PyTorch中所有神经网络模型的基类。</p>
<p><code>super(My_Model, self).__init__()</code>是一个调用父类<code>nn.Module</code>的<code>__init__</code>方法的特殊方式。让我们分解这个语法：</p>
<ol>
<li>
<p><strong><code>super()</code></strong>: 这是一个Python内置函数，用于调用父类（在这种情况下是<code>nn.Module</code>）的方法。这是在子类中调用父类的方法的推荐方式，特别是当有多重继承时。</p>
</li>
<li>
<p><strong><code>My_Model, self</code></strong>: 这是<code>super()</code>函数的两个参数。</p>
<ul>
<li><code>My_Model</code>: 这是当前的子类。</li>
<li><code>self</code>: 这是当前对象的实例。在Python中，类的方法总是接收其自身的实例作为第一个参数，通常命名为<code>self</code>。</li>
</ul>
</li>
<li>
<p><strong><code>.__init__()</code></strong>: 这是调用的方法名。在这里，我们正在调用<code>nn.Module</code>类的初始化方法<code>__init__</code>。</p>
</li>
</ol>
<ul>
<li>
<p>为什么要这样做？当您创建一个PyTorch模型并继承自<code>nn.Module</code>时，您需要确保<code>nn.Module</code>的初始化代码也被执行。这对于后续的一些PyTorch内部操作，如模型的参数注册等，是必要的。</p>
</li>
<li>
<p>至于为什么<code>self</code>有时在前，有时在后：在方法定义中，<code>self</code>总是作为第一个参数出现，表示对象本身。但在使用<code>super()</code>时，<code>self</code>通常作为第二个参数，这是Python的语法规定。</p>
</li>
</ul>
<hr>
<p>这个自定义模型类是一个典型的前馈神经网络（Feedforward Neural Network），适用于回归任务，它将输入数据通过一系列全连接层和激活函数层传递，最终产生一个输出。你可以根据具体任务的需要，修改模型的结构。</p>
<p>在PyTorch中，每次创建<code>nn.Linear</code>层时，你实际上是创建了一个新的线性层对象，而不是修改或覆盖以前的线性层。这是因为每个<code>nn.Linear</code>层都有自己的权重和偏置，它们是独立的。</p>
<p>所以，当你创建<code>nn.Linear(16, 8)</code>时，它实际上是一个新的线性层，而不会影响先前创建的<code>nn.Linear(input_dim, 16)</code>。这两个层是不同的对象，它们在模型中执行不同的转换。</p>
<p>这种分层的方法允许你创建具有不同权重和偏置的不同层，以便模型可以学习不同的特征表示。它是神经网络的关键之一，因为不同的层可以学习不同的特征并协同工作以解决问题。</p>
<hr>
<p>在选择神经网络的架构时，特别是当考虑输入层到输出层之间的层大小时，没有固定的“一刀切”的规则。选择是否使用平滑的递减、指数递减或其他策略取决于特定的应用和问题的性质。但是，我们可以提供一些建议和观点来帮助决策：</p>
<ol>
<li>
<p><strong>平滑递减</strong>:</p>
<ul>
<li><strong>优点</strong>: 可能有助于逐渐提取特征，并避免在前几层中丢失大量信息。</li>
<li><strong>缺点</strong>: 可能会增加模型的参数数量，导致更长的训练时间和可能的过拟合。</li>
</ul>
</li>
<li>
<p><strong>指数递减</strong>:</p>
<ul>
<li><strong>优点</strong>: 快速减少参数的数量，可以帮助模型更快地训练。</li>
<li><strong>缺点</strong>: 如果递减得太快，可能会在网络的前部丢失大量信息。</li>
</ul>
</li>
<li>
<p><strong>问题的性质</strong>: 对于一些问题，特征的维度可以迅速减少，因为许多输入特征可能并不都是必要的。而对于其他问题，可能需要更平滑的递减，以捕获更复杂的特征表示。</p>
</li>
<li>
<p><strong>实验与验证</strong>: 最佳的方法通常是尝试多种架构，并使用验证集来评估它们的性能。这可以帮助确定哪种策略（平滑递减、指数递减或其他）对于特定的问题最有效。</p>
</li>
<li>
<p><strong>避免过拟合</strong>: 如果网络太大，有太多的参数，它可能会过拟合训练数据。可以使用正则化技术（如dropout、L2正则化等）来对抗这个问题。</p>
</li>
<li>
<p><strong>考虑计算资源</strong>: 更大的网络意味着需要更多的计算资源来训练。根据可用的硬件，你可能需要权衡网络的大小。</p>
</li>
</ol>
<p>总之，选择网络架构是一个需要实验和迭代的过程。理想的层大小和结构可能因问题而异。在实践中，通常需要尝试多种不同的配置，并基于验证性能进行选择。</p>
<hr>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">select_feat</span>(<span class="params">train_data, valid_data, test_data, select_all=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;Selects useful features to perform regression&#x27;&#x27;&#x27;</span></span><br><span class="line">    y_train, y_valid = train_data[:,-<span class="number">1</span>], valid_data[:,-<span class="number">1</span>]</span><br><span class="line">    raw_x_train, raw_x_valid, raw_x_test = train_data[:,:-<span class="number">1</span>], valid_data[:,:-<span class="number">1</span>], test_data</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> select_all:</span><br><span class="line">        feat_idx = <span class="built_in">list</span>(<span class="built_in">range</span>(raw_x_train.shape[<span class="number">1</span>]))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        feat_idx = [<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>] <span class="comment"># <span class="doctag">TODO:</span> Select suitable feature columns.</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> raw_x_train[:,feat_idx], raw_x_valid[:,feat_idx], raw_x_test[:,feat_idx], y_train, y_valid</span><br></pre></td></tr></table></figure>
<p>这是一个用于选择特征列的函数 <code>select_feat</code>，它用于执行回归任务。以下是这个函数的逐句解释：</p>
<ol>
<li>
<p><code>def select_feat(train_data, valid_data, test_data, select_all=True):</code></p>
<ul>
<li>这是函数的定义，它接受四个参数：<code>train_data</code>，<code>valid_data</code>，<code>test_data</code>，以及一个布尔值参数 <code>select_all</code>，默认为 <code>True</code>。</li>
</ul>
</li>
<li>
<p><code>y_train, y_valid = train_data[:,-1], valid_data[:,-1]</code></p>
<ul>
<li>这两行代码从 <code>train_data</code> 和 <code>valid_data</code> 中提取目标变量（标签），通常在回归任务中，这是模型要预测的值。</li>
</ul>
</li>
<li>
<p><code>raw_x_train, raw_x_valid, raw_x_test = train_data[:,:-1], valid_data[:,:-1], test_data</code></p>
<ul>
<li>这三行代码从输入数据中分别提取特征，通过切片 <code>[:,:-1]</code> 取掉最后一列，以便获得没有目标变量的特征数据。</li>
</ul>
</li>
<li>
<p><code>if select_all: feat_idx = list(range(raw_x_train.shape[1]))</code></p>
<ul>
<li>这是一个条件语句，如果 <code>select_all</code> 为 <code>True</code>，则 <code>feat_idx</code> 将包含所有特征的索引。这意味着将使用所有可用的特征进行建模。</li>
</ul>
</li>
<li>
<p><code>else: feat_idx = [0,1,2,3,4]</code></p>
<ul>
<li>如果 <code>select_all</code> 为 <code>False</code>，则 <code>feat_idx</code> 将包含一个手动选择的特征索引列表。这是一个示例，表示你可以手动选择使用的特征列，以便建模。</li>
</ul>
</li>
<li>
<p>最后，函数返回了已选择特征的训练集、验证集和测试集，以及相应的目标变量。所选的特征列由 <code>feat_idx</code> 决定，如果 <code>select_all</code> 为 <code>True</code>，则使用所有特征。</p>
</li>
</ol>
<p>这个函数的目的是根据 <code>select_all</code> 参数，选择特征列，并返回相应的数据子集，以便用于回归任务的训练和测试。根据具体问题，你可以选择使用所有特征或手动选择感兴趣的特征列。</p>
<hr>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">trainer</span>(<span class="params">train_loader, valid_loader, model, config, device</span>):</span><br><span class="line"></span><br><span class="line">    criterion = nn.MSELoss(reduction=<span class="string">&#x27;mean&#x27;</span>) <span class="comment"># Define your loss function, do not modify this.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Define your optimization algorithm.</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Please check https://pytorch.org/docs/stable/optim.html to get more available algorithms.</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> L2 regularization (optimizer(weight decay...) or implement by your self).</span></span><br><span class="line">    optimizer = torch.optim.SGD(model.parameters(), lr=config[<span class="string">&#x27;learning_rate&#x27;</span>], momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line">    writer = SummaryWriter() <span class="comment"># Writer of tensoboard.</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.isdir(<span class="string">&#x27;./models&#x27;</span>):</span><br><span class="line">        os.mkdir(<span class="string">&#x27;./models&#x27;</span>) <span class="comment"># Create directory of saving models.</span></span><br><span class="line"></span><br><span class="line">    n_epochs, best_loss, step, early_stop_count = config[<span class="string">&#x27;n_epochs&#x27;</span>], math.inf, <span class="number">0</span>, <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(n_epochs):</span><br><span class="line">        model.train() <span class="comment"># Set your model to train mode.</span></span><br><span class="line">        loss_record = []</span><br><span class="line"></span><br><span class="line">        <span class="comment"># tqdm is a package to visualize your training progress.</span></span><br><span class="line">        train_pbar = tqdm(train_loader, position=<span class="number">0</span>, leave=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> train_pbar:</span><br><span class="line">            optimizer.zero_grad()               <span class="comment"># Set gradient to zero.</span></span><br><span class="line">            x, y = x.to(device), y.to(device)   <span class="comment"># Move your data to device.</span></span><br><span class="line">            pred = model(x)</span><br><span class="line">            loss = criterion(pred, y)</span><br><span class="line">            loss.backward()                     <span class="comment"># Compute gradient(backpropagation).</span></span><br><span class="line">            optimizer.step()                    <span class="comment"># Update parameters.</span></span><br><span class="line">            step += <span class="number">1</span></span><br><span class="line">            loss_record.append(loss.detach().item())</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Display current epoch number and loss on tqdm progress bar.</span></span><br><span class="line">            train_pbar.set_description(<span class="string">f&#x27;Epoch [<span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;n_epochs&#125;</span>]&#x27;</span>)</span><br><span class="line">            train_pbar.set_postfix(&#123;<span class="string">&#x27;loss&#x27;</span>: loss.detach().item()&#125;)</span><br><span class="line"></span><br><span class="line">        mean_train_loss = <span class="built_in">sum</span>(loss_record)/<span class="built_in">len</span>(loss_record)</span><br><span class="line">        writer.add_scalar(<span class="string">&#x27;Loss/train&#x27;</span>, mean_train_loss, step)</span><br><span class="line"></span><br><span class="line">        model.<span class="built_in">eval</span>() <span class="comment"># Set your model to evaluation mode.</span></span><br><span class="line">        loss_record = []</span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> valid_loader:</span><br><span class="line">            x, y = x.to(device), y.to(device)</span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                pred = model(x)</span><br><span class="line">                loss = criterion(pred, y)</span><br><span class="line"></span><br><span class="line">            loss_record.append(loss.item())</span><br><span class="line"></span><br><span class="line">        mean_valid_loss = <span class="built_in">sum</span>(loss_record)/<span class="built_in">len</span>(loss_record)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Epoch [<span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;n_epochs&#125;</span>]: Train loss: <span class="subst">&#123;mean_train_loss:<span class="number">.4</span>f&#125;</span>, Valid loss: <span class="subst">&#123;mean_valid_loss:<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">        writer.add_scalar(<span class="string">&#x27;Loss/valid&#x27;</span>, mean_valid_loss, step)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> mean_valid_loss &lt; best_loss:</span><br><span class="line">            best_loss = mean_valid_loss</span><br><span class="line">            torch.save(model.state_dict(), config[<span class="string">&#x27;save_path&#x27;</span>]) <span class="comment"># Save your best model</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;Saving model with loss &#123;:.3f&#125;...&#x27;</span>.<span class="built_in">format</span>(best_loss))</span><br><span class="line">            early_stop_count = <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            early_stop_count += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> early_stop_count &gt;= config[<span class="string">&#x27;early_stop&#x27;</span>]:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;\nModel is not improving, so we halt the training session.&#x27;</span>)</span><br><span class="line">            <span class="keyword">return</span></span><br></pre></td></tr></table></figure>
<p>接下来一步一步分析：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">criterion = nn.MSELoss(reduction=&#x27;mean&#x27;)</span><br></pre></td></tr></table></figure>
<p>这行代码用于定义损失函数，其中 <code>nn.MSELoss</code> 表示均方误差损失（Mean Squared Error Loss），通常用于回归任务。让我解释一下它的各个部分：</p>
<ul>
<li>
<p><code>nn.MSELoss</code>: 这部分表示你正在使用PyTorch的均方误差损失函数。均方误差是回归任务中常用的损失函数之一，它用于衡量模型的预测值与实际目标值之间的差距。</p>
</li>
<li>
<p><code>reduction='mean'</code>: 这是损失函数的一个参数，它定义了如何计算损失的标量值。在这里，设置为 <code>'mean'</code> 表示计算所有样本的均方误差，然后求平均值，得到一个标量损失值。其他可能的选项包括 <code>'sum'</code>（计算总和）和 <code>'none'</code>（不进行降维，保持与输入张量相同的维度）。</p>
</li>
</ul>
<p>所以，<code>nn.MSELoss(reduction='mean')</code> 的作用是定义了一个均方误差损失函数，它会计算模型的预测值与实际目标值之间的均方误差，并将其求平均得到一个标量损失值，用于衡量模型的性能。这是回归任务中常见的损失函数之一。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=config[<span class="string">&#x27;learning_rate&#x27;</span>], momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure>
<p>这是PyTorch中定义优化器的语句。<br>
<code>optimizer = torch.optim.SGD(model.parameters(), lr=config['learning_rate'], momentum=0.9)</code></p>
<ul>
<li>
<p><code>optimizer</code>: 是一个变量，存储了优化器的实例。</p>
</li>
<li>
<p><code>torch.optim.SGD</code>: 这是PyTorch中随机梯度下降（SGD）的实现。</p>
</li>
<li>
<p><code>model.parameters()</code>: 这会返回模型中所有的可训练参数。这些参数是我们在训练过程中需要更新的。</p>
</li>
<li>
<p><code>lr=config['learning_rate']</code>: 这设置了学习率为配置字典中的<code>'learning_rate'</code>键对应的值。学习率是一个关键的超参数，它决定了在每次更新时参数应该改变的幅度。如果学习率太高，模型可能无法收敛；如果太低，则训练可能会非常慢。</p>
</li>
<li>
<p><code>momentum=0.9</code>: 动量是SGD优化器的一个变种，它在更新时考虑了前几次的梯度，以便更快地收敛并减少震荡。动量值通常设置在0.5到0.9之间。在此，它被设置为0.9，意味着当前梯度的90%和前一次的梯度的10%将被用来更新参数。</p>
</li>
</ul>
<p>总之，这行代码创建了一个随机梯度下降优化器，该优化器用于更新模型的参数，它使用了动量来加速收敛。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">writer = SummaryWriter() # Writer of tensoboard.</span><br></pre></td></tr></table></figure>
<p>这行代码与其注释是关于TensorBoard的可视化工具的。</p>
<p><code>writer = SummaryWriter()</code></p>
<ul>
<li>
<p><code>SummaryWriter()</code>: 这是PyTorch的<code>torch.utils.tensorboard</code>中的一个类，它允许用户记录数据，例如损失、准确率等，以便稍后在TensorBoard中进行可视化。</p>
</li>
<li>
<p><code>writer</code>: 是创建的<code>SummaryWriter</code>对象的实例。之后，你可以使用这个实例来添加标量、图像、文本等到TensorBoard日志中。</p>
</li>
</ul>
<p>注释 <code># Writer of tensorboard.</code> 提供了有关这一行代码的简短描述，即这是TensorBoard的写入器。</p>
<p>TensorBoard 是一个由TensorFlow提供的可视化工具，但也可以与PyTorch一起使用。它可以帮助用户可视化模型的训练进程、结构、梯度分布等。这在调试、优化和理解模型时非常有用。使用<code>SummaryWriter</code>，你可以很容易地将这些数据记录到日志中，并在TensorBoard中查看。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_pbar = tqdm(train_loader, position=<span class="number">0</span>, leave=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>这行代码涉及<code>tqdm</code>库，它是一个用于在Python中显示进度条的库。当你有一个长时间运行的循环或任务时，<code>tqdm</code>可以为你提供一个视觉上友好的进度条，以便你知道任务的进展情况。</p>
<p>让我们分析这行代码：</p>
<p><code>train_pbar = tqdm(train_loader, position=0, leave=True)</code></p>
<ol>
<li>
<p><strong><code>tqdm(train_loader)</code></strong>:</p>
<ul>
<li><code>train_loader</code>: 是一个可迭代对象，通常是一个PyTorch数据加载器。这意味着我们将在进度条中追踪<code>train_loader</code>的迭代进度。</li>
<li>当你迭代<code>train_pbar</code>（例如在一个for循环中）时，它实际上会迭代<code>train_loader</code>，同时更新和显示进度条。</li>
</ul>
</li>
<li>
<p><strong><code>position=0</code></strong>:</p>
<ul>
<li>这定义了进度条在屏幕上的位置。当你有多个进度条或在多线程环境中使用<code>tqdm</code>时，<code>position</code>参数确保进度条在正确的位置显示。</li>
</ul>
</li>
<li>
<p><strong><code>leave=True</code></strong>:</p>
<ul>
<li>当进度条完成时，这个参数决定进度条是否仍然保留在屏幕上。如果为<code>True</code>，进度条在完成后将保留在屏幕上；如果为<code>False</code>，进度条完成后将被清除。</li>
</ul>
</li>
</ol>
<p>总的来说，这行代码创建了一个用于显示<code>train_loader</code>迭代进度的进度条，并将其保存在<code>train_pbar</code>变量中。当你在后续的代码中迭代<code>train_pbar</code>时，进度条会自动更新。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer.zero_grad()</span><br></pre></td></tr></table></figure>
<p>如果不在每个batch开始时调用<code>optimizer.zero_grad()</code>清空参数的梯度，会导致问题。具体来说，PyTorch的设计是为了累积梯度，也就是说，每当<code>.backward()</code>被调用时，梯度都会累积到现有的梯度上，而不是替换它们。</p>
<p>这种累积的特性有以下后果：</p>
<ol>
<li>
<p><strong>错误的参数更新</strong>：如果你不清空梯度，那么新的梯度值将被添加到上一个batch的梯度值上。这意味着，模型的参数更新会基于多个batch的梯度累积，而不是仅仅基于当前batch的梯度。这可能会导致模型收敛得更慢或者完全不收敛。</p>
</li>
<li>
<p><strong>梯度爆炸</strong>：累积的梯度可能会非常大，尤其是在深度网络或大数据集上。这可能会导致梯度爆炸，即梯度变得非常大，导致模型参数在训练过程中变得不稳定。</p>
</li>
<li>
<p><strong>增加的内存使用</strong>：持续的梯度累积可能会导致更大的内存占用。</p>
</li>
<li>
<p><strong>不正确的模型评估</strong>：如果你在评估模型时忘记清空梯度，并执行了<code>.backward()</code>（虽然在评估时通常不需要），那么梯度仍然会累积，这可能会影响到下一次的训练周期。</p>
</li>
</ol>
<p>为了避免这些问题，建议在每个batch开始时调用<code>optimizer.zero_grad()</code>，确保在每次前向和反向传播时都是从零开始计算梯度。</p>
<p>PyTorch选择累积梯度而不是直接替换具有其特定的原因和用途，这种设计为某些特定的模型和技术提供了灵活性：</p>
<ol>
<li>
<p><strong>支持更复杂的模型和损失结构</strong>：有时，我们可能需要从多个源头计算梯度，并将它们聚合。例如，当我们有多个损失函数时，我们可能希望分别计算每个损失的梯度，并在之后将它们相加。累积梯度允许我们这样做。</p>
</li>
<li>
<p><strong>RNNs和BPTT</strong>：对于循环神经网络（RNNs）和通过时间反向传播（BPTT），由于RNN的时间步长，我们可能希望在多个时间步上累积梯度，然后一次性更新。</p>
</li>
<li>
<p><strong>稀疏更新</strong>：对于一些特定的优化技术，我们可能不想在每个批次结束时立即更新所有的参数。通过梯度累积，我们可以在多个批次上计算梯度，然后再进行一次更大的更新。</p>
</li>
<li>
<p><strong>节省计算资源</strong>：在某些场景下，特别是当GPU内存有限时，使用更小的批次进行前向和反向传播，然后累积梯度，可以模拟更大批次的效果而不需要增加计算负担。</p>
</li>
<li>
<p><strong>支持多GPU训练</strong>：当使用多个GPU进行数据并行处理时，每个GPU可以处理一个批次的子集并计算梯度。然后，这些梯度可以在所有GPUs之间聚合（累积）。</p>
</li>
</ol>
<p>考虑到这些情况，PyTorch的设计者选择了更灵活的累积梯度方法。但这也意味着，在大多数常规训练场景中，开发者需要记住在每个批次开始时清零梯度，以避免不必要的累积。</p>
<hr>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">writer.add_scalar(&#x27;Loss/train&#x27;, mean_train_loss, step)</span><br></pre></td></tr></table></figure>
<p>这行代码使用<code>writer</code>（一个<code>SummaryWriter</code>的实例，通常用于TensorBoard日志记录）来添加一个标量值到日志中。这个标量通常用于记录和可视化模型的训练过程中的某些指标，如损失或准确率。我将为您详细解释这行代码：</p>
<p><code>writer.add_scalar('Loss/train', mean_train_loss, step)</code></p>
<ol>
<li>
<p><code>add_scalar</code>: 这是<code>SummaryWriter</code>类的一个方法，用于添加一个标量到TensorBoard日志中。</p>
</li>
<li>
<p><code>'Loss/train'</code>: 这是要添加的标量的标签或名称。在TensorBoard中，你将看到一个叫做&quot;Loss/train&quot;的图表。</p>
</li>
<li>
<p><code>mean_train_loss</code>: 这是要记录的实际数值。在此，它是当前训练周期内所有批次的平均损失。</p>
</li>
<li>
<p><code>step</code>: 这表示当前的全局步骤或迭代。它是一个整数，通常用于表示自训练开始以来已经处理过的批次数量。在TensorBoard的图表中，它将作为x轴，允许你随着时间看到损失的变化。</p>
</li>
</ol>
<p>总的来说，这行代码将当前训练周期的平均损失记录到TensorBoard的日志中，允许你在训练过程中可视化和跟踪损失的变化。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss_record.append(loss.detach().item())</span><br></pre></td></tr></table></figure>
<p>这行代码涉及了将当前批次的损失值添加到一个记录损失的列表中。我会为您逐步解析这行代码：</p>
<p><code>loss_record.append(loss.detach().item())</code></p>
<ol>
<li>
<p><code>loss_record</code>: 这是一个Python列表，用于在整个训练周期内累积每个批次的损失值。</p>
</li>
<li>
<p><code>append()</code>: 这是Python列表的一个方法，用于在列表的末尾添加一个元素。</p>
</li>
<li>
<p><code>loss</code>: 这是计算得到的损失Tensor，它还包含关于模型参数的梯度信息。</p>
</li>
<li>
<p><code>detach()</code>: 这是一个PyTorch Tensor的方法。它返回一个新的Tensor，这个新Tensor与原始Tensor共享数据但不需要梯度计算（即它不会跟踪计算历史）。这在此上下文中是有用的，因为我们只关心损失的数值，不需要保留关于损失的任何计算历史。</p>
</li>
<li>
<p><code>item()</code>: 这是一个PyTorch Tensor的方法。对于只包含一个元素的tensor，它将返回tensor中的这个值作为一个标准的Python数字。在这种情况下，它返回损失的数值。</p>
</li>
</ol>
<p>所以，这行代码的主要目的是取得当前批次的损失值，并将其添加到<code>loss_record</code>列表中，以便后续可以计算整个训练周期的平均损失或进行其他统计分析。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> mean_valid_loss &lt; best_loss:</span><br><span class="line">            best_loss = mean_valid_loss</span><br><span class="line">            torch.save(model.state_dict(), config[<span class="string">&#x27;save_path&#x27;</span>]) <span class="comment"># Save your best model</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;Saving model with loss &#123;:.3f&#125;...&#x27;</span>.<span class="built_in">format</span>(best_loss))</span><br><span class="line">            early_stop_count = <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            early_stop_count += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> early_stop_count &gt;= config[<span class="string">&#x27;early_stop&#x27;</span>]:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;\nModel is not improving, so we halt the training session.&#x27;</span>)</span><br><span class="line">            <span class="keyword">return</span></span><br></pre></td></tr></table></figure>
<p>这段代码实现了检查模型的验证损失是否改进，并根据结果决定是否保存模型和是否提前终止训练（early stopping）。下面是逐句的解释：</p>
<ol>
<li>
<p><code>if mean_valid_loss &lt; best_loss:</code></p>
<ul>
<li>判断当前的平均验证损失是否比之前记录的最佳验证损失更低。</li>
</ul>
</li>
<li>
<p><code>best_loss = mean_valid_loss</code></p>
<ul>
<li>如果当前的验证损失是更低的，那么更新<code>best_loss</code>为当前的<code>mean_valid_loss</code>。</li>
</ul>
</li>
<li>
<p><code>torch.save(model.state_dict(), config['save_path'])</code></p>
<ul>
<li>保存当前模型的参数到指定的路径。这里假定你只想保存最佳的模型，即验证损失最低的模型。</li>
</ul>
</li>
<li>
<p><code>print('Saving model with loss &#123;:.3f&#125;...'.format(best_loss))</code></p>
<ul>
<li>打印一条消息，告知用户模型已被保存，并显示最佳的验证损失。</li>
</ul>
</li>
<li>
<p><code>early_stop_count = 0</code></p>
<ul>
<li>重置早停计数器。这意味着模型在这个周期是有所改进的。</li>
</ul>
</li>
<li>
<p><code>else:</code></p>
<ul>
<li>如果模型没有改进。</li>
</ul>
</li>
<li>
<p><code>early_stop_count += 1</code></p>
<ul>
<li>早停计数器增加1。这意味着模型在这个周期没有改进。</li>
</ul>
</li>
<li>
<p><code>if early_stop_count &gt;= config['early_stop']:</code></p>
<ul>
<li>判断早停计数器是否达到预设的阈值。如果连续多个周期模型都没有改进，那么这个条件将为真。</li>
</ul>
</li>
<li>
<p><code>print('\nModel is not improving, so we halt the training session.')</code></p>
<ul>
<li>打印一条消息，告知用户模型已经连续多个周期没有改进。</li>
</ul>
</li>
<li>
<p><code>return</code></p>
<ul>
<li>提前结束<code>trainer</code>函数的执行。这将终止训练过程。</li>
</ul>
</li>
</ol>
<p>简而言之，这段代码的目的是在每个训练周期后检查模型的性能。如果模型在验证集上表现更好，它会保存模型。如果模型连续多个周期没有改进，它会终止训练。这是一个常用的策略，称为早停，可以帮助避免模型过度拟合并减少不必要的训练时间。</p>
<hr>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">device = <span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span></span><br><span class="line">config = &#123;</span><br><span class="line">    <span class="string">&#x27;seed&#x27;</span>: <span class="number">5201314</span>,      <span class="comment"># Your seed number, you can pick your lucky number. :)</span></span><br><span class="line">    <span class="string">&#x27;select_all&#x27;</span>: <span class="literal">True</span>,   <span class="comment"># Whether to use all features.</span></span><br><span class="line">    <span class="string">&#x27;valid_ratio&#x27;</span>: <span class="number">0.2</span>,   <span class="comment"># validation_size = train_size * valid_ratio</span></span><br><span class="line">    <span class="string">&#x27;n_epochs&#x27;</span>: <span class="number">3000</span>,     <span class="comment"># Number of epochs.</span></span><br><span class="line">    <span class="string">&#x27;batch_size&#x27;</span>: <span class="number">256</span>,</span><br><span class="line">    <span class="string">&#x27;learning_rate&#x27;</span>: <span class="number">1e-5</span>,</span><br><span class="line">    <span class="string">&#x27;early_stop&#x27;</span>: <span class="number">400</span>,    <span class="comment"># If model has not improved for this many consecutive epochs, stop training.</span></span><br><span class="line">    <span class="string">&#x27;save_path&#x27;</span>: <span class="string">&#x27;./models/model.ckpt&#x27;</span>  <span class="comment"># Your model will be saved here.</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这段代码主要是设置运行训练的配置参数。我将为您逐步解析这段代码：</p>
<ol>
<li>
<p><code>device = 'cuda' if torch.cuda.is_available() else 'cpu'</code></p>
<ul>
<li>这行代码决定模型和数据应该在哪个设备上运行。如果CUDA（GPU支持）可用，它将选择GPU（<code>'cuda'</code>），否则它将选择CPU（<code>'cpu'</code>）。</li>
</ul>
</li>
<li>
<p><code>config = &#123;...&#125;</code></p>
<ul>
<li>定义一个名为<code>config</code>的字典，其中包含多个训练参数和设置。</li>
</ul>
</li>
<li>
<p><code>'seed': 5201314</code></p>
<ul>
<li>设置一个随机数种子，确保实验的可重复性。</li>
</ul>
</li>
<li>
<p><code>'select_all': True</code></p>
<ul>
<li>一个布尔标志，表示是否使用所有特征进行训练。</li>
</ul>
</li>
<li>
<p><code>'valid_ratio': 0.2</code></p>
<ul>
<li>表示验证集的大小是训练集大小的20%。</li>
</ul>
</li>
<li>
<p><code>'n_epochs': 3000</code></p>
<ul>
<li>指定训练的总周期数。</li>
</ul>
</li>
<li>
<p><code>'batch_size': 256</code></p>
<ul>
<li>在每次迭代中，模型将看到的数据批次的大小。</li>
</ul>
</li>
<li>
<p><code>'learning_rate': 1e-5</code></p>
<ul>
<li>设置学习率，它决定了在训练中参数更新的幅度。</li>
</ul>
</li>
<li>
<p><code>'early_stop': 400</code></p>
<ul>
<li>早停策略的参数。如果模型在连续400个周期中没有改进，训练将终止。</li>
</ul>
</li>
<li>
<p><code>'save_path': './models/model.ckpt'</code></p>
</li>
</ol>
<ul>
<li>定义了模型保存的路径。</li>
</ul>
<p>这个<code>config</code>字典为后续的训练步骤提供了一种方便的方式来访问和管理所有的配置参数。在实际项目中，使用这样的配置字典或配置文件是很常见的，因为它使代码更加整洁，并允许更容易地更改参数。</p>
<hr>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Set seed for reproducibility</span></span><br><span class="line">same_seed(config[<span class="string">&#x27;seed&#x27;</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># train_data size: 2699 x 118 (id + 37 states + 16 features x 5 days)</span></span><br><span class="line"><span class="comment"># test_data size: 1078 x 117 (without last day&#x27;s positive rate)</span></span><br><span class="line">train_data, test_data = pd.read_csv(<span class="string">&#x27;./covid.train.csv&#x27;</span>).values, pd.read_csv(<span class="string">&#x27;./covid.test.csv&#x27;</span>).values</span><br><span class="line">train_data, valid_data = train_valid_split(train_data, config[<span class="string">&#x27;valid_ratio&#x27;</span>], config[<span class="string">&#x27;seed&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print out the data size.</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;&quot;&quot;train_data size: <span class="subst">&#123;train_data.shape&#125;</span></span></span><br><span class="line"><span class="string">valid_data size: <span class="subst">&#123;valid_data.shape&#125;</span></span></span><br><span class="line"><span class="string">test_data size: <span class="subst">&#123;test_data.shape&#125;</span>&quot;&quot;&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Select features</span></span><br><span class="line">x_train, x_valid, x_test, y_train, y_valid = select_feat(train_data, valid_data, test_data, config[<span class="string">&#x27;select_all&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print out the number of features.</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;number of features: <span class="subst">&#123;x_train.shape[<span class="number">1</span>]&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">train_dataset, valid_dataset, test_dataset = COVID19Dataset(x_train, y_train), \</span><br><span class="line">                                            COVID19Dataset(x_valid, y_valid), \</span><br><span class="line">                                            COVID19Dataset(x_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Pytorch data loader loads pytorch dataset into batches.</span></span><br><span class="line">train_loader = DataLoader(train_dataset, batch_size=config[<span class="string">&#x27;batch_size&#x27;</span>], shuffle=<span class="literal">True</span>, pin_memory=<span class="literal">True</span>)</span><br><span class="line">valid_loader = DataLoader(valid_dataset, batch_size=config[<span class="string">&#x27;batch_size&#x27;</span>], shuffle=<span class="literal">True</span>, pin_memory=<span class="literal">True</span>)</span><br><span class="line">test_loader = DataLoader(test_dataset, batch_size=config[<span class="string">&#x27;batch_size&#x27;</span>], shuffle=<span class="literal">False</span>, pin_memory=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>这段代码涉及数据处理和准备阶段，确保数据能够正确地被模型所使用。下面我会为您逐句解释这段代码：</p>
<ol>
<li>
<p><code># Set seed for reproducibility</code></p>
<ul>
<li>这是一个注释，说明接下来的代码是为了确保实验的可重复性。</li>
</ul>
</li>
<li>
<p><code>same_seed(config['seed'])</code></p>
<ul>
<li>调用一个名为<code>same_seed</code>的函数来设置随机数种子，以确保实验的可重复性。</li>
</ul>
</li>
<li>
<p><code>train_data, test_data = pd.read_csv('./covid.train.csv').values, pd.read_csv('./covid.test.csv').values</code></p>
<ul>
<li>使用pandas库读取训练和测试数据，并将其转化为NumPy数组。</li>
</ul>
</li>
<li>
<p><code>train_data, valid_data = train_valid_split(train_data, config['valid_ratio'], config['seed'])</code></p>
<ul>
<li>使用<code>train_valid_split</code>函数将原始训练数据分为训练数据和验证数据。</li>
</ul>
</li>
<li>
<p>打印训练数据、验证数据和测试数据的大小。</p>
</li>
<li>
<p><code>x_train, x_valid, x_test, y_train, y_valid = select_feat(train_data, valid_data, test_data, config['select_all'])</code></p>
<ul>
<li>使用<code>select_feat</code>函数从原始数据中选择特定的特征。</li>
</ul>
</li>
<li>
<p>打印所选择的特征的数量。</p>
</li>
<li>
<p>创建三个数据集：训练数据集、验证数据集和测试数据集。这里使用了名为<code>COVID19Dataset</code>的自定义数据集类。</p>
</li>
<li>
<p>使用PyTorch的<code>DataLoader</code>创建数据加载器，它们可以按批次加载数据，并提供其他功能，如数据打乱和内存管理。</p>
</li>
</ol>
<ul>
<li><code>shuffle=True</code>：在训练和验证加载器中使用，表示在每个训练周期中都要打乱数据。</li>
<li><code>pin_memory=True</code>：当使用GPU时，这可以加速数据从CPU传输到GPU的过程。</li>
</ul>
<p>总之，这段代码的目的是从CSV文件中加载数据、处理数据、选择特征，并为训练、验证和测试准备数据加载器。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pd.read_csv(<span class="string">&#x27;./covid.train.csv&#x27;</span>).values</span><br></pre></td></tr></table></figure>
<p>这行代码使用pandas库来读取CSV文件，并将其转换为NumPy数组。我会为您详细解释：</p>
<ol>
<li>
<p><code>pd.read_csv('./covid.train.csv')</code>: 使用pandas的<code>read_csv</code>函数从指定的路径（<code>'./covid.train.csv'</code>）读取CSV文件。这将返回一个pandas的DataFrame对象，它是一个二维标签化的数据结构。</p>
</li>
<li>
<p><code>.values</code>: 这是一个DataFrame的属性，它会返回DataFrame中的数据作为NumPy数组。这对于进一步的数值处理或与需要NumPy数组的库（如PyTorch、Scikit-learn等）的互操作性很有用。</p>
</li>
</ol>
<p>所以，这行代码的结果是将<code>'./covid.train.csv'</code>文件中的数据读入一个NumPy数组。这个数组可以直接用于大多数数据处理和机器学习任务。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">test_loader = DataLoader(test_dataset, batch_size=config[<span class="string">&#x27;batch_size&#x27;</span>], shuffle=<span class="literal">False</span>, pin_memory=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>这行代码使用PyTorch的<code>DataLoader</code>来创建一个数据加载器，这个加载器用于按批次加载测试数据。我会为您逐步解析这行代码：</p>
<p><code>test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False, pin_memory=True)</code></p>
<ol>
<li>
<p><code>DataLoader</code>: 这是PyTorch提供的一个类，它允许您按批次加载数据，并提供其他功能，如数据打乱、多线程数据加载等。</p>
</li>
<li>
<p><code>test_dataset</code>: 这是您要加载的数据集，通常是一个PyTorch <code>Dataset</code>实例。</p>
</li>
<li>
<p><code>batch_size=config['batch_size']</code>: 这定义了每个批次的大小。<code>config['batch_size']</code>从先前定义的<code>config</code>字典中获取批次大小。</p>
</li>
<li>
<p><code>shuffle=False</code>: 这意味着在加载数据时不应随机打乱数据。这在测试时是常见的，因为您通常只想按原始顺序处理和评估测试数据。</p>
</li>
<li>
<p><code>pin_memory=True</code>: 这是一个优化，当您使用GPU时，它可以加速数据从CPU传输到GPU的过程。在内部，它会为数据预留一个固定的、锁定的（或“钉住的”）内存区域，这使得数据传输更加高效。</p>
</li>
</ol>
<p>总之，这行代码创建了一个测试数据的加载器，这个加载器按照指定的批次大小加载数据，不会打乱数据，并优化了数据的内存管理，特别是在使用GPU时。</p>
<p><code>batch_size</code>是训练神经网络时的一个关键超参数，它决定了每次参数更新所使用的样本数量。<code>batch_size</code>的选择对训练的效率、效果以及模型的性能都有影响。下面列举了<code>batch_size</code>选择大或小时的一些差异和影响：</p>
<ol>
<li>
<p><strong>计算效率</strong>:</p>
<ul>
<li><strong>大</strong>: 利用矩阵运算的并行性，特别是在GPU上，大批次可以提高计算的效率。</li>
<li><strong>小</strong>: 更频繁地进行参数更新，可能会导致硬件资源（特别是GPU）的利用率不足。</li>
</ul>
</li>
<li>
<p><strong>内存使用</strong>:</p>
<ul>
<li><strong>大</strong>: 需要更多的内存来存储大批次的数据和中间计算结果。</li>
<li><strong>小</strong>: 使用更少的内存，允许在内存较小的设备上进行训练。</li>
</ul>
</li>
<li>
<p><strong>收敛速度</strong>:</p>
<ul>
<li><strong>大</strong>: 每个周期需要更少的参数更新，可能导致收敛速度变慢。</li>
<li><strong>小</strong>: 更频繁的参数更新可能使模型更快地收敛。</li>
</ul>
</li>
<li>
<p><strong>训练稳定性</strong>:</p>
<ul>
<li><strong>大</strong>: 噪音较小，梯度估计更为准确，训练路径平滑。</li>
<li><strong>小</strong>: 梯度估计可能存在更多的噪音，可能导致训练路径不稳定。</li>
</ul>
</li>
<li>
<p><strong>泛化能力</strong>:</p>
<ul>
<li><strong>大</strong>: 有研究显示，超大批次可能导致模型的泛化能力下降。</li>
<li><strong>小</strong>: 小批次或随机梯度下降（SGD，即<code>batch_size=1</code>）经常被视为具有某种正则化效果，可能提高模型的泛化能力。</li>
</ul>
</li>
<li>
<p><strong>卡在局部最小值</strong>:</p>
<ul>
<li><strong>大</strong>: 由于梯度估计较为准确，可能更容易卡在非最优的局部最小值。</li>
<li><strong>小</strong>: 更多的噪音可能帮助模型跳出某些局部最小值。</li>
</ul>
</li>
<li>
<p><strong>调整学习率</strong>:</p>
<ul>
<li><strong>大</strong>: 使用大批次可能需要对学习率进行调整，例如增大学习率。</li>
<li><strong>小</strong>: 较小的学习率可能更适合小批次。</li>
</ul>
</li>
</ol>
<p>总的来说，<code>batch_size</code>的选择需要考虑多个因素，并可能需要实验来确定最佳值。在实践中，中等大小的批次（如32、64或128）往往是一个好的起点。</p>
<ol>
<li>
<p><strong>超大批次可能导致模型的泛化能力下降</strong>:</p>
<p>泛化能力是指模型在未见过的数据上的表现。理论上，当使用小批次时，由于每批数据都带有一定的噪音，模型在训练过程中会看到更多的“噪声”，这种噪声可以起到某种正则化的效果，有助于提高模型的泛化能力。另一方面，当使用大批次时，梯度估计更为准确，噪音更少，这可能导致模型过度拟合训练数据，从而降低其在测试数据上的性能。实际上，一些研究发现，使用超大批次可以迅速减少训练损失，但可能会导致测试损失下降得较慢或不如预期。</p>
</li>
<li>
<p><strong>大批次与学习率的关系</strong>:</p>
<p>学习率定义了每次参数更新的幅度。当使用大批次时，梯度估计基于更多的数据，因此它们通常更稳定和准确。因此，可以使用较大的学习率而不会导致训练不稳定。另外，由于大批次训练的收敛速度可能较慢（因为每个周期的更新次数减少了），增大学习率也可以帮助加速收敛。</p>
<p>另一方面，当使用小批次时，梯度估计可能会带有更多的噪音，因此较小的学习率可以帮助确保训练的稳定性，尤其是在训练的早期阶段。</p>
<p>这也解释了为什么许多自适应学习率算法，如Adam，能够在各种批次大小下都工作得很好，因为它们根据梯度的统计信息动态地调整学习率。</p>
</li>
</ol>
<p>综上所述，选择合适的批次大小和学习率是训练深度学习模型时的关键考虑因素，它们之间的关系也是深度学习研究和实践中的一个重要话题。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">save_pred</span>(<span class="params">preds, file</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27; Save predictions to specified file &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(file, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">        writer = csv.writer(fp)</span><br><span class="line">        writer.writerow([<span class="string">&#x27;id&#x27;</span>, <span class="string">&#x27;tested_positive&#x27;</span>])</span><br><span class="line">        <span class="keyword">for</span> i, p <span class="keyword">in</span> <span class="built_in">enumerate</span>(preds):</span><br><span class="line">            writer.writerow([i, p])</span><br><span class="line"></span><br><span class="line">model = My_Model(input_dim=x_train.shape[<span class="number">1</span>]).to(device)</span><br><span class="line">model.load_state_dict(torch.load(config[<span class="string">&#x27;save_path&#x27;</span>]))</span><br><span class="line">preds = predict(test_loader, model, device)</span><br><span class="line">save_pred(preds, <span class="string">&#x27;pred.csv&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>这段代码涉及加载一个训练好的模型、对测试数据进行预测，然后保存预测结果到CSV文件。我会为您逐步解析这段代码：</p>
<ol>
<li>
<p><code>def save_pred(preds, file):</code></p>
<ul>
<li>这是定义一个名为<code>save_pred</code>的函数，用于保存预测结果。它接受两个参数：<code>preds</code>是预测值的列表或数组，<code>file</code>是要保存的文件名。</li>
</ul>
</li>
<li>
<p>在函数内部：</p>
<ul>
<li>使用<code>with open(file, 'w') as fp:</code>，它打开指定的文件以供写入。</li>
<li><code>csv.writer(fp)</code>创建一个CSV写入器对象。</li>
<li>使用<code>writer.writerow()</code>来写入行到CSV文件。</li>
</ul>
</li>
<li>
<p><code>model = My_Model(input_dim=x_train.shape[1]).to(device)</code></p>
<ul>
<li>创建一个模型实例并将其移动到指定的计算设备。</li>
</ul>
</li>
<li>
<p><code>model.load_state_dict(torch.load(config['save_path']))</code></p>
<ul>
<li>使用PyTorch的<code>torch.load</code>函数从先前指定的路径加载模型的状态字典（即模型的参数）。</li>
<li>然后使用<code>load_state_dict</code>方法将这些参数加载到模型中。</li>
</ul>
</li>
<li>
<p><code>preds = predict(test_loader, model, device)</code></p>
<ul>
<li>调用一个名为<code>predict</code>的函数（虽然该函数在给定的代码片段中未定义，但我们可以假设它执行模型的前向传递并返回预测）。</li>
<li>它使用提供的测试数据加载器、模型和设备进行预测。</li>
</ul>
</li>
<li>
<p><code>save_pred(preds, 'pred.csv')</code></p>
<ul>
<li>使用先前定义的<code>save_pred</code>函数将预测结果保存到<code>pred.csv</code>文件中。</li>
</ul>
</li>
</ol>
<p>总的来说，这段代码描述了如何从磁盘加载训练好的模型，使用该模型对测试数据进行预测，然后将这些预测保存到CSV文件中。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.load_state_dict(torch.load(config[<span class="string">&#x27;save_path&#x27;</span>]))</span><br></pre></td></tr></table></figure>
<p>这行代码是PyTorch中加载预训练模型权重的常见方式。让我们逐部分解析这行代码：</p>
<ol>
<li>
<p><strong><code>torch.load(config['save_path'])</code></strong>:</p>
<ul>
<li>使用PyTorch的<code>torch.load</code>方法从指定的路径加载数据。这通常是一个包含模型参数的字典，该字典在保存模型时使用<code>torch.save</code>方法创建。</li>
<li><code>config['save_path']</code>是模型权重文件的路径。它从之前定义的<code>config</code>字典中获取。</li>
</ul>
</li>
<li>
<p><strong><code>model.load_state_dict(...)</code></strong>:</p>
<ul>
<li><code>load_state_dict</code>是PyTorch模型（或<code>nn.Module</code>实例）的一个方法，它负责将一个状态字典加载到模型中。</li>
<li>这个状态字典包含模型的权重。当我们调用<code>load_state_dict</code>方法时，它会为模型中的每个层设置适当的权重。</li>
</ul>
</li>
</ol>
<p>总之，<code>model.load_state_dict(torch.load(config['save_path']))</code>这行代码的目的是从指定路径加载模型权重，并将这些权重应用到模型<code>model</code>中。这允许你继续使用之前训练过的模型，而不必重新从头开始训练。</p>
<hr>
<p><code>Adam</code>和<code>SGD</code>都是用于优化深度学习模型的梯度下降算法，但它们的工作原理有所不同。下面是它们之间的主要区别，以及各自的优缺点：</p>
<h3 id="adam-adaptive-moment-estimation">Adam (Adaptive Moment Estimation):</h3>
<p><strong>工作原理</strong>:</p>
<ul>
<li>结合了<code>Momentum</code>和<code>RMSprop</code>的优点。</li>
<li>使用梯度的一阶矩（mean）和二阶矩（uncentered variance）来自适应地调整每个参数的学习率。</li>
</ul>
<p><strong>优点</strong>:</p>
<ol>
<li><strong>自适应学习率</strong>: Adam会为每个参数自动调整学习率，这使得它对初始学习率和超参数选择的敏感性较低。</li>
<li><strong>快速收敛</strong>: 由于Momentum的效果，Adam往往收敛得相对较快。</li>
<li><strong>适合大数据和参数</strong>: 对于大型数据集和模型参数，Adam通常表现得很好。</li>
</ol>
<p><strong>缺点</strong>:</p>
<ol>
<li><strong>内存占用</strong>: 由于Adam需要为每个参数存储一阶和二阶矩估计，因此它的内存使用量通常比SGD大。</li>
<li><strong>可能过拟合</strong>: 在某些情况下，特别是数据集较小的情况下，Adam可能会过拟合。</li>
</ol>
<h3 id="sgd-stochastic-gradient-descent">SGD (Stochastic Gradient Descent):</h3>
<p><strong>工作原理</strong>:</p>
<ul>
<li>在每次迭代中使用一部分（或一个）数据样本的梯度来更新模型参数。</li>
</ul>
<p><strong>优点</strong>:</p>
<ol>
<li><strong>简单和高效</strong>: SGD的实现很简单，计算效率也很高。</li>
<li><strong>泛化</strong>: 在某些情况下，尤其是当正则化技巧与SGD结合使用时，SGD可能会得到更好的泛化性能。</li>
</ol>
<p><strong>缺点</strong>:</p>
<ol>
<li><strong>需要仔细选择学习率</strong>: SGD的收敛通常对初始学习率和学习率调度策略非常敏感。</li>
<li><strong>慢速收敛</strong>: 相比于使用自适应学习率技术的优化器，SGD可能需要更多的迭代次数才能收敛。</li>
<li><strong>可能遇到局部最小值或鞍点</strong>: 在某些非凸优化问题中，SGD可能会在局部最小值或鞍点附近停滞。</li>
</ol>
<h3 id="总结">总结:</h3>
<ul>
<li><strong>Adam</strong>是一个自适应学习率的优化器，通常在不需要手动微调超参数的情况下就可以快速收敛。对于快速原型设计和大型数据集/模型，它往往是一个很好的选择。</li>
<li><strong>SGD</strong>虽然简单，但在某些情况下可能会提供更好的泛化性能。对于深度学习竞赛和高级研究项目，仍然值得考虑使用和微调SGD。</li>
</ul>
<p>在实际应用中，建议尝试多种优化器，看看哪种方法在特定任务上表现最好。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="https://detect42.github.io">Richard</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://detect42.github.io/post/a43615c7.html">https://detect42.github.io/post/a43615c7.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><a class="post-meta__tags" href="/tags/GPT/">GPT</a><a class="post-meta__tags" href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">神经网络</a></div><div class="post_share"><div class="social-share" data-image="/img/ff.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/post/e31a8f3c.html" title="NTU_ML_HW1"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">NTU_ML_HW1</div></div></a></div><div class="next-post pull-right"><a href="/post/97fe6ca1.html" title="人工智能导论HW2黑白棋实验报告"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">人工智能导论HW2黑白棋实验报告</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/post/79ac928f.html" title="Accelerating Techniques for self-Attention"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-12-29</div><div class="title">Accelerating Techniques for self-Attention</div></div></a></div><div><a href="/post/e0d2147a.html" title="Transformer"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-12-29</div><div class="title">Transformer</div></div></a></div><div><a href="/post/b867d47d.html" title="Batch Normalization"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-12-28</div><div class="title">Batch Normalization</div></div></a></div><div><a href="/post/f63ef346.html" title="Word Embedding"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-12-28</div><div class="title">Word Embedding</div></div></a></div><div><a href="/post/d145174e.html" title="Recurrent Neural Network(RNN)"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-12-27</div><div class="title">Recurrent Neural Network(RNN)</div></div></a></div><div><a href="/post/e31a8f3c.html" title="NTU_ML_HW1"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-10-19</div><div class="title">NTU_ML_HW1</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#sample-code-for-pytorch"><span class="toc-text">Sample code for PyTorch</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B3%E4%BA%8E%E9%9A%8F%E6%9C%BA%E6%95%B0%E7%94%9F%E6%88%90%E5%99%A8"><span class="toc-text">关于随机数生成器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#adam-adaptive-moment-estimation"><span class="toc-text">Adam (Adaptive Moment Estimation):</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#sgd-stochastic-gradient-descent"><span class="toc-text">SGD (Stochastic Gradient Descent):</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-text">总结:</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image: url('/img/topimg.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By Richard</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></body></html>