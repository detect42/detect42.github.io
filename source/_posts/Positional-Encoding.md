---
title: Positional Encoding
tags: ml
abbrlink: 2cc570d5
date: 2025-06-23 10:15:07
---

## Positional Encoding

### [参考博文](https://fleetwood.dev/posts/you-could-have-designed-SOTA-positional-encoding)

# 从位置编码到 RoPE：设计思考与演进过程综述

在 Transformer 模型中，**自注意力机制**本质上是不关心输入顺序的。这一特点虽然使得模型并行计算成为可能，但同时也带来了一个问题：如何让模型区分同一词在不同位置上的不同语义？解决这一问题便是**位置编码**的初衷。

## 为什么需要位置编码

Transformer 模型在处理输入序列时，每个 token 的嵌入初始都是独立的，没有顺序信息。为了解决这一缺陷，早期的设计者们选择了一种简单直接的方法——**将位置编码与词嵌入相加**。这种加法操作使得最终传入多头自注意力计算矩阵始终保持统一维度，同时把位置信息“融入”到了语义空间中，使得模型无须额外学习如何整合两种信息。[¹]

## 从加法到拼接：取舍的背后

虽然理论上可以考虑通过拼接来将位置信息与语义信息组合，但大部分设计者更青睐于加法操作。原因在于：
- **维度一致性**：加法仅仅是在原始嵌入上做一个“细微的调整”，保持了每个向量的长度不变；而拼接会增加维度，导致后续各层的参数和计算设计都需要随之调整。
- **特征融合**：通过相加，模型在处理点积运算（如自注意力中的 Q 和 K 相乘）的同时，就能将语义和位置信息自然融合。如果采用拼接，信息则分散在不同的子空间中，模型还必须学会如何将其整合，这无疑增加了训练难度和不确定性。

这些实践经验促使人们进一步思考：我们能否设计出一种在不污染语义表示的前提下，又能自然地融入位置信息的方案？

## 早期思考：整数编码与二进制编码

最初的直觉可能是直接**将 token 的位置整数值**添加到嵌入中，但由于位置数值往往远大于嵌入本身的尺度，这种方法很容易造成信息的“淹没”。于是，有人尝试**对位置数值进行归一化**，甚至转换成**二进制形式**，将每一位扩展到与嵌入维度对应的位置上。
- **整数编码**：简单粗暴，但数值量级问题严重，语义和位置信号混杂。
- **二进制编码**：在数值范围上得到了改善，但二进制编码天然的不连续性和离散性，使得向量之间的变化“跳跃”较大，难以训练出平滑、连续的模型表征。[¹]

## 平滑过渡：Sinusoidal（正余弦）编码的出现

观察二进制编码的不足之后，设计者们转而思考是否有更适合连续函数的选项。正弦（sin）和余弦（cos）函数以其周期性和平滑性成为理想候选。
- **Sinusoidal 编码**：原理上，每个位置对应一个由 sin 和 cos 函数构成的向量，其中不同维度的周期成几何级数变化。这不仅能**唯一标示每个位置**，而且还隐含了两个位置之间的**相对关系**，使得模型能够捕捉到 token 间距离的细微变化。
- 数学上，这种编码满足平移性质：通过对 sin 与 cos 应用加法公式，我们能够证明这种方法实际上已经在编码中引入了一种**二维旋转结构**。[¹]

## 灵感乍现：旋转矩阵与相对位置

在对 Sinusoidal 编码做深入分析时，有一个洞察闪现：
- **正余弦函数组合本质上就是一个二维旋转矩阵**。
数学上，给定一个固定的旋转角度，旋转矩阵能够将任意向量旋转 k 步后产生一个符合预期变化的结果，同时保持向量的范数不变。
- 这种性质正好契合了我们对于位置编码的要求——**在不破坏语义空间（保持嵌入范数不变）的同时，传递位置信息**，使得相同语义的词在不同位置上的区分性由旋转角度自然体现出来。

## Rotary Positional Encoding (RoPE) 的诞生

在以上思路的启发下，RoPE 概念逐渐浮现。原有的位置编码方案都是在 token 嵌入层面进行“加法式”叠加，而 RoPE 则采用了一种**乘法式的旋转**策略。
- **核心操作**：在计算自注意力的 Q 和 K 之前，先将它们的向量分块（通常以 2D 为一组），然后为每一组应用一个与位置 p 相关的旋转矩阵。
- 这样，不同位置的 Q 和 K 在进行点积运算前，都已经通过旋转体现了彼此的相对位置，使得相邻 token 的点积能自然捕捉到“距离”的信息，而语义信息却依然以原始范数存在，不受干扰。
- **优势**：RoPE 能够保持语义与位置信息的解耦，同时具备良好的扩展性——不仅适用于传统文本数据，也可扩展到图像（2D）、脑电图（4D）等更高维度的数据场景。[¹]

## 总结

从最初简单的整数或二进制编码，到利用 sin/cos 提供平滑连续的 Sinusoidal 编码，再到深入分析其内在旋转机制，整个过程展示了设计者们如何从问题出发、不断迭代、用数学工具剖析问题背后蕴含的几何意义。最终，RoPE 的提出不仅使得位置编码在数值上更为精妙，还有效地解决了语义与位置信息如何“共存”的矛盾。这一切都证明了：**在深度学习模型中，深入的数学洞察往往能够引领我们走向更优雅、更高效的设计方案**。

这种从“加法”叠加到“乘法”旋转的转变，不仅是技术细节上的提升，更是对模型理解和信息融合机制的一次深刻革新。未来，我们也许还会见到更多结合信号处理、分层编码等思想的新方法，但 RoPE 为现代 Transformer 模型提供了一条兼具理论优雅与实践高效的道路。