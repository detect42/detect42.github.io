{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55ebed44-a2d2-4965-9048-7d4caece6d82",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 1. ÂáÜÂ§áÁéØÂ¢É"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9bdcf5-f21e-40f2-8979-548cb1db6ba5",
   "metadata": {},
   "source": [
    "### 1.1 ÂÆâË£Ö‰æùËµñ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f66b4b-15f8-47fa-a6c0-70795799ad31",
   "metadata": {},
   "source": [
    "Áé∞Âú®ÔºåËÆ©Êàë‰ª¨ÂÆâË£Ö‰∏Ä‰∫õÈ¢ùÂ§ñÁöÑÂ∫ìÔºå‰æãÂ¶Ç langchain Âíå python-dotenv„ÄÇ\n",
    "\n",
    "ÂâçËÄÖ‰∏∫Êàë‰ª¨Êèê‰æõ‰∫Ü‰∏Ä‰∏™ÊûÑÂª∫Âü∫‰∫éLLMÁöÑÂ∫îÁî®Á®ãÂ∫èÁöÑÊ®°ÂùóÂåñÊ°ÜÊû∂ÔºåËÄåÂêéËÄÖÂú®‰∏∫Âú®Á∫øLLMÊúçÂä°ËÆæÁΩÆAPIÂØÜÈí•ÊñπÈù¢‰∏∫Êàë‰ª¨ËäÇÁúÅ‰∫ÜÊó∂Èó¥ÔºàÊúâÂÖ≥ËØ¶ÁªÜ‰ø°ÊÅØÔºåËØ∑ÂèÇËßÅ‰∏ã‰∏ÄËäÇÔºâ„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0608b18-2faf-40f8-beaf-efbd713e4f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install langchain, the library we will learn during our courses\n",
    "!pip install langchain==0.0.338 -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892e98f2-bbf1-4c8f-a755-5a8c64e9a405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dotenv, auto-load environment variables from `.env` files\n",
    "!pip install python-dotenv==1.0.0 -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2919c726-32d3-4025-9e8f-db00e5ca1c74",
   "metadata": {},
   "source": [
    "Ê≠§Â§ñÔºåËÆ©Êàë‰ª¨ÂÆâË£ÖÁî®‰∫éÂØπÂÜÖÂÆπËøõË°åÊ†áËÆ∞ÂåñÂíåÂ≠òÂÇ®Âú®ÂêëÈáèÊï∞ÊçÆÂ∫ì‰∏äÁöÑÂ∫ìÔºåÂç≥ tiktoken Âíå faiss-cpu„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3e332c-0324-42ca-b5ca-8be73cfea0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install tiktoken, the library used by OpenAI models for tokenizing text strings\n",
    "!pip install tiktoken==0.5.1 -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55858b86-2ff1-46a7-a473-78295437ecd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install faiss-cpu, a vector database for storing content along with embedding vectors\n",
    "!pip install faiss-cpu==1.7.4 -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2865033a-0683-4b6a-a478-8f6b9d120765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install wikipedia, the library for accessing wikipedia service in code\n",
    "!pip install wikipedia==1.4.0 -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd5f5f4-63d3-441d-8c9a-f7976f33555c",
   "metadata": {},
   "source": [
    "ÁÑ∂ÂêéÔºåÂÆâË£Ö‰∏Ä‰∫õÁî®‰∫éËÆøÈóÆÂ§ñÈÉ®ÊúçÂä°ÁöÑÂ∫ìÔºå‰æãÂ¶Ç wikipedia„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a14613b-82a4-40b2-8c82-770d5cb7b63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install wikipedia, the library for accessing wikipedia service in code\n",
    "!pip install wikipedia==1.4.0 -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3711d50-d9ac-44b3-b7c6-25b084474119",
   "metadata": {},
   "source": [
    "ÊúÄÂêéÔºå‰∏∫‰∫ÜÊµãËØïÂÆâË£ÖÂíåAPIÂØÜÈí•ÁöÑÊúâÊïàÊÄßÔºåÊàë‰ª¨ËøòÂÆâË£ÖÁõ∏Â∫î‰æõÂ∫îÂïÜÁöÑSDKÂ∫ìÔºàÂç≥OpenAIÂíåÊô∫Ë∞±AIÔºâ„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2623724c-492a-4737-be44-0ae014214042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install openai, official SDK by OpenAI for invoking GPT models\n",
    "!pip install openai==1.3.3 -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1338b17d-d953-4742-9fed-e8a2080170b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install zhipu, official SDK by OpenAI for invoking ChatGLM models\n",
    "!pip install zhipuai==1.0.7 -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06972013-340f-427f-9038-12cde927c9a7",
   "metadata": {},
   "source": [
    "### 1.2 ÁéØÂ¢ÉÂèòÈáè"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6cfdeb-fdbf-4a09-a8fa-0ed98e470606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['ZHIPUAI_API_KEY']='replace_with_your_zhipuai_api_key_here'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5017855e-c925-4264-9319-e47ebcbe250b",
   "metadata": {},
   "source": [
    "### 1.3 ÊµãËØïÂáÜÂ§áÊòØÂê¶ÊàêÂäü"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af19cdeb-20c5-44aa-85b2-6df9422bf5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test zhipuai installation\n",
    "import os\n",
    "import zhipuai\n",
    "\n",
    "zhipuai.api_key = os.getenv('ZHIPUAI_API_KEY')  # Set API key from envrionment variable\n",
    "\n",
    "prompt = \"\"\"You will be provided with a sentence in English, and your task is to translate it into Chinese.\n",
    "\n",
    "My name is Jane. What is yours?\n",
    "\"\"\"\n",
    "\n",
    "completion = zhipuai.model_api.invoke(\n",
    "    model='chatglm_turbo',\n",
    "    prompt=[\n",
    "        {'role': 'user', 'content': prompt}\n",
    "    ],\n",
    "    temperature=0.,\n",
    ")\n",
    "\n",
    "print(completion['data']['choices'][0]['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e0678d-8625-41ce-b335-cb09274c868d",
   "metadata": {},
   "source": [
    "## 2. LangchainÂü∫Á°ÄÁªÉ‰π†ÔºàÂü∫‰∫éÊô∫Ë∞±LLMÔºâ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b07742-bfc3-4e59-888f-f5619a9fe247",
   "metadata": {},
   "source": [
    "‰∏éOpenAI‰∏çÂêåÔºåLangChainÂπ∂‰∏çÂéüÁîüÊîØÊåÅÊô∫Ë∞±AIÁöÑÂú®Á∫øLLMÊúçÂä°„ÄÇÁõ∏ÂèçÔºåÊàë‰ª¨ÂèØ‰ª•ÁºñÂÜô‰∏Ä‰∏™ÂåÖË£ÖÁ±ªÊù•Â∞ÜÊô∫Ë∞±AIÁöÑChatGLPÊúçÂä°ÁßªÊ§çÂà∞LangChainÔºåËøôË¶ÅÂΩíÂäü‰∫éLangChainÁöÑÊ®°ÂùóÂåñÊé•Âè£„ÄÇËøôÂ∫îËØ•Á±ª‰ºº‰∫éÊàë‰ª¨‰ΩøÁî®OpenAIÁöÑGPTÊúçÂä°Êó∂ÁöÑÊÑüËßâ„ÄÇ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c752ae-14ff-451b-a30d-cb1190340da2",
   "metadata": {},
   "source": [
    "### 2.1 Ê£ÄÊü•ZhipuAI wrapperÊòØÂê¶Â≠òÂú®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c40040-163c-4df3-a869-15a4fa931043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check ZhipuAI wrapper existence\n",
    "!ls -la | grep \"zhipuai\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f8e2d6-86c1-4900-8398-fa25a3559aea",
   "metadata": {},
   "source": [
    "### 2.2 ÁÆÄÂçï‰ΩøÁî®‰æãÂ≠ê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bd3d2f-b446-42a6-9920-7ad218dfc003",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zhipuai_llm import ZhipuAILLM\n",
    "\n",
    "prompt = \"\"\"You will be provided with a sentence in English, and your task is to translate it into Chinese.\n",
    "\n",
    "My name is Jane. What is yours?\n",
    "\"\"\"\n",
    "\n",
    "llm = ZhipuAILLM(model='chatglm_turbo', temperature=0.)\n",
    "\n",
    "response = llm.predict(prompt)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb88ab0-0ba3-4140-a939-ae91f39b4521",
   "metadata": {},
   "source": [
    "#### ÁªÉ‰π†1 - \"ËÆ°ÁÆóÊó∂Èó¥Â§çÊùÇÂ∫¶\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dc11a5-6e42-4419-b3f6-ad8d0f2fa105",
   "metadata": {},
   "source": [
    "> üí™ Practice yourself.\n",
    "> Please finish the code for this task, with the following prompt example:\n",
    ">\n",
    "> ---------------------------\n",
    "> \n",
    "> ```\n",
    "> You will be provided with Python code, and your task is to calculate its time complexity.\n",
    ">\n",
    "> def foo(n, k):\n",
    ">    accum = 0\n",
    ">    for i in range(n):\n",
    ">        for l in range(k):\n",
    ">            accum += i\n",
    ">    return accum\n",
    "> ```\n",
    "> \n",
    "> ---------------------------\n",
    "> Try to change the Python code for analysis and see how LLM responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d167350b-bca4-4bd0-9a15-0f10e8d6187d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e19a3a-dd46-464b-8612-c103dd0e3427",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### ÁªÉ‰π†2 - ‚ÄúÂæÆÂçöÊÉÖÊÑüÂàÜÊûê‚Äù"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641d45ed-cc60-4d04-a4c6-6719721e4889",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "> üí™ Practice yourself.\n",
    "> Please finish the code for this task, with the following prompt example:\n",
    ">\n",
    "> ---------------------------\n",
    "> ```\n",
    "> You will be provided with a tweet, and your task is to classify its sentiment as \n",
    "> positive, neutral, or negative.\n",
    "> \n",
    "> I loved the new Batman movie!\n",
    "> ```\n",
    ">\n",
    "> ---------------------------\n",
    "> Try to change the tweet text for analysis and see how LLM responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e4b7ba-2bb0-46cc-9407-7855b101710c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45f1a19-cabc-4801-9178-9024748a8a8d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### ÁªÉ‰π†3 - ‚ÄúÊú∫Âú∫‰ª£Âè∑ÊèêÂèñ‚Äù"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf980e2b-c9c0-4cdf-8b44-062027f8fa8c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "> üí™ Practice yourself.\n",
    "> Please finish the code for this task, with the following prompt example:\n",
    ">\n",
    "> ---------------------------\n",
    "> ```\n",
    "> You will be provided with a text, and your task is to extract the airport codes from it.\n",
    "> \n",
    "> I want to fly from Orlando to Boston\n",
    "> ```\n",
    ">\n",
    "> ---------------------------\n",
    "> Try to change the city names and see how LLM responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a10b019-a071-407a-8e4e-0fb13cdbc2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09091de-7a0a-4338-918c-cca0608a2397",
   "metadata": {},
   "source": [
    "### 2.3 Êé¢Á¥¢LLMÂ±ÄÈôê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e6a1b5-e44e-47ff-9584-8d3040106eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zhipuai_llm import ZhipuAILLM\n",
    "\n",
    "prompt = \"\"\"Which team won the 1986 FIFA World Cup?\"\"\"\n",
    "llm = ZhipuAILLM(model='chatglm_turbo', temperature=0.)\n",
    "response = llm.predict(prompt)\n",
    "print(f'- 1st response: {response}')\n",
    "\n",
    "prompt = \"\"\"Which team won the 2022 FIFA World Cup?\"\"\"\n",
    "llm = ZhipuAILLM(model='chatglm_turbo', temperature=0.)\n",
    "response = llm.predict(prompt)\n",
    "print(f'- 2nd response: {response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d69741-cf70-453b-abb8-b865aee1e07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zhipuai_llm import ZhipuAILLM\n",
    "\n",
    "prompt = \"\"\"Sum 4829 and 2930, and then multiply by 1923.\"\"\"\n",
    "\n",
    "llm = ZhipuAILLM(model='chatglm_turbo', temperature=0.)\n",
    "response = llm.predict(prompt)\n",
    "\n",
    "print(f'- gpt: {response}')\n",
    "print(f'- truth:\\n\\n {(4829 + 2930) * 1923}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d15a3b-39e5-4420-b81e-75a4aed444e0",
   "metadata": {},
   "source": [
    "### 2.4 Êé¢Á¥¢LangchainÊ®°ÂùóÂåñÁªÑ‰ª∂ËÆæËÆ°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72490709-13f7-4fd1-9fa6-6fed0c61e876",
   "metadata": {},
   "source": [
    "üìå ÊâìÂºÄË∞ÉËØïÂíåËØ¶ÁªÜÊ®°Âºè\n",
    "\n",
    "Â¶ÇÊûúÊÇ®ÊòØÂàùÂ≠¶ËÄÖÔºåÊàë‰ª¨Âª∫ËÆÆÊÇ®Âú®LangChain‰∏≠ÊâìÂºÄË∞ÉËØïÂíåËØ¶ÁªÜÊ®°ÂºèÔºåÂú®LLMÂ∫îÁî®Á®ãÂ∫èÊâßË°åËøáÁ®ã‰∏≠ÊòæÁ§∫‰∏≠Èó¥Ê≠•È™§ÁöÑÈ¢ùÂ§ñ‰ø°ÊÅØ„ÄÇ\n",
    "Êü•ÁúãÊèêÁ§∫Â¶Ç‰ΩïÂ°´ÂÖÖ‰ª•Âèä‰∏≠Èó¥LLMÁîüÊàêÁöÑÂìçÂ∫îÊòØ‰∏™Â•Ω‰∏ªÊÑèÔºàÂú®Ê≠£Â∏∏Ê®°Âºè‰∏ã‰∏çÂ∫îÊâìÂç∞‰ªª‰ΩïËæìÂá∫Ôºâ„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6857b586-03a6-4700-a1a5-83b3e034a6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "\n",
    "langchain.debug = True\n",
    "langchain.verbose = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556ee3a0-2cf6-4dd8-9a4e-f1dfb70227db",
   "metadata": {},
   "source": [
    "#### Model I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa2ad97-46ce-424a-afb7-a94b7ef674ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "from zhipuai_llm import ZhipuAILLM\n",
    "\n",
    "# [1] Custom output parser, split comma separated strings and return as list\n",
    "class CommaSeparatedListOutputParser(BaseOutputParser):\n",
    "    \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\"\n",
    "\n",
    "    def parse(self, text: str):\n",
    "        \"\"\"Parse the output of an LLM call.\"\"\"\n",
    "        return text.strip().split(\", \")\n",
    "\n",
    "# [2] System message template, declare task requirement as prompt\n",
    "template = \"\"\"You are a helpful assistant who generates comma separated lists.\n",
    "A user will pass in a category, and you should generate 5 objects in that category in a comma separated list.\n",
    "ONLY return a comma separated list, and nothing more.\"\"\"\n",
    "\n",
    "# [3] Human message template, here we use Python format string syntax\n",
    "# (https://docs.python.org/3/library/string.html#formatstrings)\n",
    "human_template = '{text}'\n",
    "\n",
    "# [4] We send both messages to LLM for response\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', template),\n",
    "    ('human', human_template),\n",
    "])\n",
    "\n",
    "# [5] Build up simple chain with LangChain Expression Language\n",
    "# (https://python.langchain.com/docs/expression_language/)\n",
    "chain = chat_prompt | ZhipuAILLM(model='chatglm_turbo') | CommaSeparatedListOutputParser()\n",
    "\n",
    "# [6] Call simple chain with human input, i.e., text = \"colors\"\n",
    "chain.invoke({'text': 'colors'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbd5c10-9981-474c-9ad2-288ede059f10",
   "metadata": {},
   "source": [
    "#### Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf40377-0d36-4250-9585-41dc0a63ea56",
   "metadata": {},
   "source": [
    "Âú®Êé•‰∏ãÊù•ÁöÑÈÉ®ÂàÜÔºåÊàë‰ª¨Â∞Ü‰∏ìÊ≥®‰∫é‰º†ÁªüÁöÑChainÊé•Âè£„ÄÇÈ¶ñÂÖàÂºÄÂßãÈáçÂÜôÂâç‰∏ÄËäÇ‰∏≠ÁöÑICELÈ£éÊ†ºÈìæ„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652675c6-df3b-426e-b6b6-97efc878af1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "\n",
    "from zhipuai_llm import ZhipuAILLM\n",
    "\n",
    "template = \"\"\"You are a helpful assistant who generates comma separated lists.\n",
    "A user will pass in a category, and you should generate 5 objects in that category in a comma separated list.\n",
    "ONLY return a comma separated list, and nothing more.\"\"\"\n",
    "\n",
    "human_template = '{text}'\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', template),\n",
    "    ('human', human_template),\n",
    "])\n",
    "\n",
    "# Equivalent to `chain = chat_prompt | ZhipuAILLM(model='chatglm_turbo') | CommaSeparatedListOutputParser()`\n",
    "chain = LLMChain(\n",
    "    llm=ZhipuAILLM(model='chatglm_turbo'),\n",
    "    prompt=chat_prompt,\n",
    "    output_parser=CommaSeparatedListOutputParser(),\n",
    ")\n",
    "\n",
    "chain.invoke({'text': 'colors'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d039d6c-ad90-4b0b-bbba-1a71588878d6",
   "metadata": {},
   "source": [
    "ÁÑ∂ÂêéÔºåËÆ©Êàë‰ª¨Áúã‰∏Ä‰∏™Êõ¥Â§çÊùÇÁöÑÈìæ„ÄÇÊàë‰ª¨Â∞Ü‰ªãÁªç‰∏Ä‰∏™ÁÆÄÂçïÁöÑ‰∏§Èò∂ÊÆµËøûÁª≠ÈìæÔºåÂÖ∂‰∏≠Ôºö\n",
    "\n",
    "1. ‰∏∫‰∏ÄÂÆ∂Âà∂ÈÄ†ÊüêÁßç‰∫ßÂìÅÁöÑÂÖ¨Âè∏ÊèêÂá∫ÂêçÁß∞\n",
    "2. ‰∏∫ÊèêÂá∫ÁöÑÂÖ¨Âè∏ÂÜô‰∏Ä‰∏™ÁÆÄÁü≠ÁöÑÊèèËø∞ÔºàÂç≥Âè£Âè∑Ôºâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8bcd6b-f5e7-44f0-90bd-192e352c91a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "\n",
    "from zhipuai_llm import ZhipuAILLM\n",
    "\n",
    "product = 'Pure Milk'\n",
    "\n",
    "# [0] The same LLM instance shared by both chains (remember LLM is stateless)\n",
    "llm = ZhipuAILLM(model='chatglm_turbo', temperature=0.7)\n",
    "\n",
    "# [1] Build name chain (1st chain)\n",
    "name_template = \"\"\"What is the best name to describe a company that makes {product}?\"\"\"\n",
    "name_prompt = ChatPromptTemplate.from_template(name_template)\n",
    "name_chain = LLMChain(llm=llm, prompt=name_prompt)\n",
    "\n",
    "# [2] Build slogan chain (2nd chain)\n",
    "slogan_template = \"\"\"Write a 20 words slogan for the following company:{company_name}\"\"\"\n",
    "slogan_prompt = ChatPromptTemplate.from_template(slogan_template)\n",
    "slogan_chain = LLMChain(llm=llm, prompt=slogan_prompt)\n",
    "\n",
    "# [3] Construct final chain in a sequencial manner\n",
    "overall_chain = SimpleSequentialChain(chains=[name_chain, slogan_chain])\n",
    "\n",
    "# [4] Call our final chain to propose and write slogan\n",
    "overall_chain.run(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cee8a2-5574-4f87-9d45-8f2161edd5b6",
   "metadata": {},
   "source": [
    "#### Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316dca0f-aef3-4a36-8edc-d68d55c40893",
   "metadata": {},
   "source": [
    "ÂõûÈ°æ‰∏Ä‰∏ãÊàë‰ª¨ËØ¥ËøáÁöÑLLMÊú¨Ë¥®‰∏äÊòØÊó†Áä∂ÊÄÅÁöÑÔºåÂç≥ÂêéÁª≠Ë∞ÉÁî®Ê∞∏Ëøú‰∏ç‰ºöÂõûÂøÜËµ∑Âú®‰πãÂâçÁöÑË∞ÉÁî®‰∏≠ÊèêÂà∞ÁöÑ‰ø°ÊÅØ„ÄÇËÆ©Êàë‰ª¨Áúã‰∏Ä‰∏™‰æãÂ≠êÊù•ËØ¥ÊòéËøô‰∏™ËØ¥Ê≥ï„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06801d81-a3f2-46e1-a7db-1a33bba1b53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zhipuai_llm import ZhipuAILLM\n",
    "\n",
    "llm = ZhipuAILLM(model='chatglm_turbo', temperature=0.7)\n",
    "print(f'Initial message: {llm.predict(\"Hello, my name is Charles.\")}')\n",
    "print(f'Follow-up message: {llm.predict(\"Well, what is my name?\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9780d238-cf65-4356-bbc6-797acef6fed1",
   "metadata": {},
   "source": [
    "Áé∞Âú®ÔºåËÆ©Êàë‰ª¨ÁúãÁúãÂ¶Ç‰ΩïÂú®LangChain‰∏≠‰∏∫‰∏Ä‰∏™ÂØπËØùÂ∫îÁî®Á®ãÂ∫èÊ∑ªÂä†‰∏Ä‰∏™ËÆ∞ÂøÜÊ®°Âùó„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÊàë‰ª¨Â∞Ü‰ΩøÁî®ConversationBufferMemoryËÆ∞ÂøÜÊ®°Âùó„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1731ea5-edd8-42c3-9a3b-a4d1ccc02913",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from zhipuai_llm import ZhipuAILLM\n",
    "\n",
    "# [1] Notice that \"chat_history\" is present in the prompt template\n",
    "template = \"\"\"You are a nice chatbot having a conversation with a human.\n",
    "\n",
    "Previous conversation:\n",
    "{chat_history}\n",
    "\n",
    "New human question: {question}\n",
    "Response:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# [2] Notice that we need to align the `memory_key`\n",
    "memory = ConversationBufferMemory(memory_key='chat_history')\n",
    "\n",
    "llm = ZhipuAILLM(model='chatglm_turbo', temperature=0.7)\n",
    "\n",
    "# [3] Memory should work with Chain for effect\n",
    "chain = LLMChain(llm=llm, prompt=prompt, memory=memory)\n",
    "\n",
    "print(f'Initial message: {chain.invoke(\"Hello, my name is Charles.\")[\"text\"]}')\n",
    "print(f'Follow-up message: {chain.invoke(\"Well, what is my name?\")[\"text\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0b293b-93c1-4c4c-9b41-e0ecda1d0c31",
   "metadata": {},
   "source": [
    "#### Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe394b53-6399-4e64-9a51-a9544a62590e",
   "metadata": {},
   "source": [
    "Áé∞Âú®ÔºåËÆ©Êàë‰ª¨Áúã‰∏Ä‰∏™ÁÆÄÂçïÁöÑÊ£ÄÁ¥¢ÊñπÂºèÔºåÂç≥Âü∫‰∫éÂêëÈáèÂ≠òÂÇ®ÁöÑÊ£ÄÁ¥¢Âô®ÔºåÂπ∂ÁúãÁúãÂÆÉÂú®LangChainÁªÑ‰ª∂‰∏≠ÁöÑÂ∑•‰ΩúÂéüÁêÜ„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496c96be-ce08-4cb3-8b2c-6cc0d5b060a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "from zhipuai_embedding import ZhipuAIEmbeddings\n",
    "\n",
    "# [1] Load content from disk file\n",
    "loader = TextLoader('ÊµÅÊµ™Âú∞ÁêÉ.txt')\n",
    "documents = loader.load()\n",
    "\n",
    "# [2] Transform file content into splits for storage and retrieve\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# [3] Here we invoke embedding functions provided by OpenAI services, which maps text\n",
    "#     string of any size into a fixed size embedding vector, where similar text are\n",
    "#     mapped into vectors of short distance\n",
    "# [4] We use FAISS as our vector store backend to save content along with embedding vectors\n",
    "embeddings = ZhipuAIEmbeddings()\n",
    "db = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "# [5] Retriever can be directly accessed from vector store instance\n",
    "retriever = db.as_retriever()\n",
    "docs = retriever.get_relevant_documents(\"ÊµÅÊµ™Âú∞ÁêÉËÆ°Âàí\")\n",
    "\n",
    "# [6] Interate around retrieved documents and print first 100 characters of each\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f'doc #{i}: {doc.page_content[:100]}...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958c0779-3941-46ac-955e-944f708a23bc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 2.5 LangChain: Hands-On ÁªÉ‰π†4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2780749e-1242-4815-92ce-ec87d6617ed9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Âú®Êú¨ËäÇ‰∏≠ÔºåÊàë‰ª¨Â∞ÜÂÄüÂä©LangChainÊ°ÜÊû∂ÊûÑÂª∫‰∏Ä‰∏™ÁÆÄÂçïÁöÑLLMÂ∫îÁî®Á®ãÂ∫è„ÄÇÊàë‰ª¨Âç≥Â∞ÜÊûÑÂª∫ÁöÑÂ∫îÁî®Á®ãÂ∫èÊòØ‰∏Ä‰∏™ÊñáÊ°£ËÅäÂ§©Êú∫Âô®‰∫∫ÔºåÂÖÅËÆ∏ÊÇ®Â∞±ÊñáÊ°£Êñá‰ª∂ÁöÑÂÜÖÂÆπÊèêÂá∫ÈóÆÈ¢ò„ÄÇÊúâÂÖ≥Êõ¥Â§ö‰ø°ÊÅØÔºåËØ∑ÂèÇÈòÖ[Chatbot](https://python.langchain.com/docs/use_cases/chatbots)„ÄÇ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946a0149-d6a9-4ce2-bec8-17c549ba5bd4",
   "metadata": {},
   "source": [
    "**step1:**\n",
    "\n",
    "ËÆ©Êàë‰ª¨È¶ñÂÖàÂÆö‰πâË¶Å‰ΩøÁî®ÁöÑLLMÊ®°Âûã„ÄÇ‰∏é‰ª•Ââç‰∏ÄÊ†∑ÔºåÂèØ‰ª•‰ΩøÁî®Êô∫Ë∞±AI„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8eb571-9c5c-4df0-a945-0485da2806e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4ab12d-7617-48ed-95fa-99058b316f5e",
   "metadata": {},
   "source": [
    "**step2:**\n",
    "  \n",
    "ÁÑ∂ÂêéÔºåÂàõÂª∫‰∏Ä‰∏™Áî®‰∫éÂ≠òÂÇ®ÂéÜÂè≤ËÅäÂ§©Ê∂àÊÅØÁöÑËÆ∞ÂøÜÔºåËøô‰ΩøÂæóËÅäÂ§©Êú∫Âô®‰∫∫ËÉΩÂ§üËÆ∞‰ΩèÂÖàÂâçÁöÑÂØπËØù„ÄÇÂú®ËøôÈáåÔºå‰∏çÂÜç‰ΩøÁî®‰πãÂâçÁöÑConversationBufferMemoryÔºåËÄåÊòØÂ∞ùËØïÂè¶‰∏ÄÁßçËÆ∞ÂøÜÔºåÂç≥ConversationSummaryMemory„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db120e36-3347-4cf8-9eeb-7123b0e69f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0727dc7-45d1-41ab-b008-b2f2a3174ff9",
   "metadata": {},
   "source": [
    "Ê≥®ÊÑèÔºåConversationSummaryMemoryÊé•Âèó‰∏Ä‰∏™Âêç‰∏∫llmÁöÑÂèÇÊï∞„ÄÇ\n",
    "\n",
    "ÂÆûÈôÖ‰∏äÔºåËøô‰∏™ËÆ∞ÂøÜ‰øùÁïô‰∫Ü‰∏§ÁßçÁ±ªÂûãÁöÑÂéÜÂè≤ÂØπËØù‰ø°ÊÅØÔºåÂç≥ÂéÜÂè≤Ê∂àÊÅØÁöÑÂàóË°®ÂíåÂéÜÂè≤Ê∂àÊÅØÁöÑÁÆÄÁü≠ÊëòË¶Å„ÄÇ\n",
    "\n",
    "‰∏éConversationBufferMemoryÁõ∏ÊØîÔºåÊëòË¶ÅÁöÑ‰ΩøÁî®‰ΩøÊàë‰ª¨‰∏ç‰ºö‰ΩøLLM‰∏ä‰∏ãÊñáÁ™óÂè£Ôºà‰ª§ÁâåÈôêÂà∂ÔºâÂèòÂæóËáÉËÇø„ÄÇ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470e4d11-4386-4cb2-8460-ed7af7d0f60c",
   "metadata": {},
   "source": [
    "**step3:**\n",
    "\n",
    "‰πãÂêéÔºåËÆ©Êàë‰ª¨ÂÆåÊàêÊ£ÄÁ¥¢Âô®ÈÉ®ÂàÜÔºåÂç≥Âä†ËΩΩÊñáÊ°£„ÄÅÊãÜÂàÜÊñáÊú¨„ÄÅËΩ¨Êç¢‰∏∫ÂµåÂÖ•Âπ∂Â≠òÂÇ®Âú®Êï∞ÊçÆÂ∫ì‰∏≠„ÄÇ\n",
    "  \n",
    "‰∏é‰πãÂâç‰∏ÄÊ†∑ÔºåÊàë‰ª¨Â∞Ü‰ΩøÁî®FAISSÂêëÈáèÂ≠òÂÇ®„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fce06be-b6ea-477c-90f3-825d577168eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loadÊï∞ÊçÆËµÑÊ∫ê\n",
    "# Write your code here\n",
    "blog_url = 'https://lilianweng.github.io/posts/2023-06-23-agent/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfa9dbe-8cb9-4bbe-a4e5-d1b542e891ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÊãÜÂàÜÊï∞ÊçÆÊàêÂùó\n",
    "# Write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b684fe-7e26-431c-81a5-4ca01b7eec58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÂêëÈáèÂ§ÑÁêÜÂ≠òÂÖ•ÂêëÈáèÊï∞ÊçÆÂ∫ì\n",
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228e21d4-6c61-4e48-a65b-e9647b662255",
   "metadata": {},
   "source": [
    "**step4:**\n",
    "ÊúÄÂêéÔºåËÆ©Êàë‰ª¨Â∞Ü‰∏äËø∞ÁªÑ‰ª∂ÁªÑÂêàÊàê‰∏Ä‰∏™Âçï‰∏ÄÁöÑÈìæ„ÄÇÊàë‰ª¨‰ΩøÁî®ÁöÑÈìæÊòØ`ConversationalRetrievalChain`„ÄÇËØ•ÈìæÁöÑÂ∑•‰ΩúÊñπÂºèÂ¶Ç‰∏ãÔºö\n",
    "\n",
    "1. ‰ΩøÁî®ËÅäÂ§©ÂéÜÂè≤ÂíåÊñ∞ÈóÆÈ¢òÂàõÂª∫‰∏Ä‰∏™‚ÄúÁã¨Á´ãÈóÆÈ¢ò‚Äù„ÄÇ\n",
    "2. Â∞ÜËøô‰∏™Êñ∞ÈóÆÈ¢ò‰º†ÈÄíÁªôÊ£ÄÁ¥¢Âô®ÔºåÂπ∂ËøîÂõûÁõ∏ÂÖ≥ÊñáÊ°£„ÄÇ\n",
    "3. Â∞ÜÊ£ÄÁ¥¢Âà∞ÁöÑÊñáÊ°£‰∏éÊñ∞ÈóÆÈ¢òÔºàÈªòËÆ§Ë°å‰∏∫ÔºâÊàñÂéüÂßãÈóÆÈ¢òÂíåËÅäÂ§©ÂéÜÂè≤‰∏ÄËµ∑‰º†ÈÄíÁªôLLMÔºåÁîüÊàêÊúÄÁªàÁöÑÂìçÂ∫î„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33837b92-bc2f-4a5e-aa56-7dbb6445af04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283e45e2-ab5c-4677-b75f-d9f796b50081",
   "metadata": {},
   "source": [
    "**step5:**\n",
    "  \n",
    "Áé∞Âú®ÔºåËÆ©Êàë‰ª¨ÊµãËØï‰∏Ä‰∏ãÊàë‰ª¨ÁöÑËÅäÂ§©Êú∫Âô®‰∫∫„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295bb80e-f4c8-4ffe-ac01-4e2b97371748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question One: 'How do agents use Task decomposition?'\n",
    "# Write your code here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5de5ec0-54bb-44cc-8f32-b0d8e4f57844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question Two: 'What are the various ways to implement memory to support it?'\n",
    "# Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a79a5a-670c-425b-b287-a0bd77f12acc",
   "metadata": {},
   "source": [
    "## 3. Âü∫‰∫éLLMÁöÑAgentÔºàÂü∫‰∫éOpenAIÔºâ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bfe2e3-6903-46b8-ab75-99443ce79605",
   "metadata": {},
   "source": [
    "**Agent: Hands-On**\n",
    " \n",
    "AgentsÁöÑÊ†∏ÂøÉÊÄùÊÉ≥ÊòØ‰ΩøÁî®ËØ≠Ë®ÄÊ®°ÂûãÈÄâÊã©‰∏ÄÁ≥ªÂàóË¶ÅÊâßË°åÁöÑÂä®‰Ωú„ÄÇ\n",
    "\n",
    "ËÄåÂú®Chains‰∏≠Ôºå‰∏ÄÁ≥ªÂàóÂä®‰ΩúÊòØÁ°¨ÁºñÁ†ÅÁöÑÔºàÂú®‰ª£Á†Å‰∏≠Ôºâ\n",
    "\n",
    "Âú®Agent‰∏≠ÔºåËØ≠Ë®ÄÊ®°ÂûãË¢´Áî®‰ΩúÊé®ÁêÜÂºïÊìéÔºåÁ°ÆÂÆöË¶ÅÊâßË°åÂì™‰∫õÂä®‰Ωú‰ª•ÂèäÈ°∫Â∫è„ÄÇ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c49329a-2433-49c0-8951-a2f8ae61443a",
   "metadata": {},
   "source": [
    "‰∏∫‰∫ÜÊîØÊåÅÊûÑÂª∫Âü∫‰∫éLLMÁöÑAgentÔºâÔºåLangChainÊèê‰æõ‰∫Ü‰ª•‰∏ãÊ®°ÂùóÂåñÁªÑ‰ª∂ÔºåÂç≥\n",
    "\n",
    "* `Tool`ÔºöÂåÖË£Ö‰∫Ü‰∏Ä‰∏™PythonÂáΩÊï∞ÂíåÁõ∏Â∫îÁöÑÊñáÊú¨ÊèèËø∞ÔºåÂÆÉËµã‰∫àAgentË∞ÉÁî®Â§ñÈÉ®Â∑•ÂÖ∑ÁöÑËÉΩÂäõÔºå‰æãÂ¶ÇËÆ°ÁÆóÂô®„ÄÅPythonËß£ÈáäÂô®„ÄÅÊêúÁ¥¢ÂºïÊìéAPI„ÄÇ\n",
    "* `Agent`ÔºöÊâ©Â±ï‰∫ÜÊôÆÈÄöÁöÑLangChain`Chain`Ê®°ÂùóÔºåÂÖ∑Êúâ‰∏ÄÁªÑ`Tool`Ôºå‰ª•ÂèäÁî®‰∫é‰∏≠Èó¥Ê≠•È™§ÁöÑÊèêÁ§∫Ôºà‰æãÂ¶ÇReAct‰ª£ÁêÜÁöÑ‚ÄúÊÄùËÄÉ/Âä®‰Ωú/ËßÇÂØü‚ÄùËøΩË∏™ÔºâÔºå‰ª£ÁêÜÊâßË°åÁöÑËæìÂá∫Ë¶Å‰πàÊòØË¶ÅÈááÂèñÁöÑ‰∏ã‰∏Ä‰∏™Âä®‰ΩúÔºà`AgentAction`ÔºâÔºåË¶Å‰πàÊòØÂèëÈÄÅÁªôÁî®Êà∑ÁöÑÊúÄÁªàÂìçÂ∫îÔºà`AgentFinish`Ôºâ„ÄÇ\n",
    "* `AgentExecutor`ÔºöÊòØAgentÁöÑËøêË°åÊó∂ÔºåÂÆÉÂÆûÈôÖ‰∏äË∞ÉÁî®`Agent`ÔºåÊâßË°åÂÆÉÈÄâÊã©ÁöÑÂä®‰ΩúÔºåÂ∞ÜÂä®‰ΩúÁöÑËæìÂá∫‰º†ÈÄíÂõûAgentÔºåÁÑ∂ÂêéÈáçÂ§çÔºåÁõ¥Âà∞ËææÂà∞`AgentFinish`„ÄÇ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43c2490-2e78-45ac-afaa-52fe2f4f7947",
   "metadata": {},
   "source": [
    "> ‚ùó ÂáÜÂ§áÊÇ®ÁöÑAPIÂØÜÈí•\n",
    ">\n",
    "> Á°Æ‰øùÊÇ®Â∑≤ÁªèÊåâÁÖßÂÖàÂÜ≥Êù°‰ª∂ËÆæÁΩÆ‰∫ÜÂºÄÂèëÁéØÂ¢ÉÔºåÂπ∂Êã•ÊúâË∞ÉÁî®LLMÊúçÂä°ÁöÑÊúâÊïàAPIÂØÜÈí•ÔºåËøôÈáå‰ª•OpenAI‰∏∫‰æã„ÄÇ\n",
    ">\n",
    "> ËØ∑Á°Æ‰øùÊÇ®Â∑≤Áªè‰ªéÁéØÂ¢ÉÂèòÈáè‰∏≠Âä†ËΩΩ‰∫ÜOpenAPIÂØÜÈí•‰ª•‰æõ‰ΩøÁî®ÔºåÂ¶Ç‰∏ãÊâÄÁ§∫„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603e0e5e-937d-4b6b-9236-556f2b54bc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY']='replace_with_your_open_api_key_here'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d56076b-6b0f-460a-a7af-39a34ca3154c",
   "metadata": {},
   "source": [
    "### 3.1 Tool: Python Function + Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cc7f6d-5f81-4074-a75b-8d3a6979f80f",
   "metadata": {},
   "source": [
    "È¶ñÂÖàÔºåËÆ©Êàë‰ª¨Áúã‰∏Ä‰∏ãLangChainÁé∞ÊàêÊèê‰æõÁöÑ‰∏Ä‰∫õÂÜÖÁΩÆTool„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65cc613-fb87-45e4-90ac-ea16e93d8277",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numexpr -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63423193-4fee-44bc-9ee8-abacea1e954c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# [1] Some tools rely on LLM during its execution\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0.7)\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents.load_tools import get_all_tool_names\n",
    "\n",
    "math_tools = load_tools(['llm-math'], llm=llm)  # [1] Tool for arithmetic calculation\n",
    "meteo_tools = load_tools(['open-meteo-api'], llm=llm)  # [2] Tool for weather info\n",
    "wiki_tools = load_tools(['wikipedia'])  # [3] Tool for searching on Wikipedia\n",
    "\n",
    "# [4] Print total list of builtin tool names\n",
    "print(get_all_tool_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc69c6b5-b8bb-4217-ab30-7133da36993d",
   "metadata": {},
   "source": [
    "#### `llm_math`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c018ad41-7e01-4002-8c0f-3b8df5a84405",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_math = math_tools[0]\n",
    "\n",
    "# [1] Try a simple equation.\n",
    "print(f'LLM Math: 2 + 2 => {llm_math.run(\"What is 2 + 2?\")}')\n",
    "\n",
    "# [2] How about a slightly diffucult one? Recall that pure LLM may fail on this example.\n",
    "print(f'LLM Math: (4829 + 2930) * 1923 => {llm_math.run(\"Sum 4829 and 2930, and then multiply by 1923.\")}')\n",
    "\n",
    "# [3] Pure LLM failed to reach the correct answer.\n",
    "print(f'Pure LLM: \\n{llm.predict(\"Sum 4829 and 2930, and then multiply by 1923.\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4ec366-cb1a-4fa4-9547-54d17a3761c8",
   "metadata": {},
   "source": [
    "#### `open-meteo-api`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8c7563-6a31-4333-a5b2-9312ab4ef08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo = meteo_tools[0]\n",
    "print(meteo.run(\"What's the weather in Paris?\"))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0c0e8e80-b1f6-4227-b5f2-3215f980ed18",
   "metadata": {},
   "source": [
    "ÁÑ∂ÂêéÔºåÈô§‰∫Ü‰ΩøÁî®LangChainÊèê‰æõÁöÑÂÜÖÁΩÆToolÔºåÊàë‰ª¨ËøòÂèØ‰ª•ÂÆö‰πâËá™Â∑±ÁöÑÂ∑•ÂÖ∑Ôºå‰ª•‰æø‰ΩøÁî®ÁÆÄÂçïÁöÑ‰ª£Á†Å„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbfecfa-6da0-47b1-9f92-7e04a830b64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import tool\n",
    "from datetime import date\n",
    "\n",
    "@tool  # [1] We use the `tool` decorator to create new `Tool` instance\n",
    "def time(text: str) -> str:\n",
    "    # [2] The docstring (wrapped in \"\"\" \"\"\") are used as tool description\n",
    "    #     (which is sent to LLM when used by agent)\n",
    "    \"\"\"Returns todays date, use this for any \\\n",
    "    questions related to knowing todays date. \\\n",
    "    The input should always be an empty string, \\\n",
    "    and this function will always return todays \\\n",
    "    date - any date mathmatics should occur \\\n",
    "    outside this function.\"\"\"\n",
    "    return str(date.today())  # [3] The actual logic for this `Tool`, i.e, return today's date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a412d745-de60-4321-86f4-a59a4e7fa1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.run('')  # Note the input is not used in our customed `Tool`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0504426-27d8-4c3d-a143-cb6a419ae12c",
   "metadata": {},
   "source": [
    "Âè¶‰∏Ä‰∏™Ëá™ÂÆö‰πâÂ∑•ÂÖ∑ÔºåÂÆÉÊé•ÂèóÂ§ö‰∏™ÂèÇÊï∞‰Ωú‰∏∫ËæìÂÖ•Âπ∂ËøîÂõû‰∏Ä‰∏™Âçï‰∏ÄÁöÑÂ≠óÁ¨¶‰∏≤„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde986d1-1517-41fc-b214-f5f60baf8bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from langchain.tools import tool\n",
    "import requests\n",
    "\n",
    "@tool\n",
    "def post_message(url: str, body: dict, parameters: Optional[dict] = None) -> str:\n",
    "    \"\"\"Sends a POST request to the given url with the given body and parameters.\"\"\"\n",
    "    result = requests.post(url, json=body, params=parameters)\n",
    "    return f\"Status: {result.status_code} - {result.text}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0e9415-cd30-437b-b302-3fa458751a7d",
   "metadata": {},
   "source": [
    "### 3.2  Agent: Chain Equipped with Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f20e32a-9e08-4709-af24-ab0f22b97c17",
   "metadata": {},
   "source": [
    "LangChainÂ∑≤ÁªèÂÆö‰πâ‰∫Ü‰∏Ä‰∫õÂÜÖÁΩÆÁöÑAgentÁ±ªÂûãÔºåÊàë‰ª¨ÂèØ‰ª•Áõ¥Êé•Âú®ÂÖ∂Âü∫Á°Ä‰∏äÊûÑÂª∫Êàë‰ª¨ÁöÑÂ∫îÁî®Á®ãÂ∫è„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe993ae0-9c7e-4be1-b853-24f2cc79c090",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.types import AgentType\n",
    "print([item.name for item in AgentType])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c7ce7e-c657-40f8-80a8-71aae8a9fa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ËÆ©Êàë‰ª¨Áúã‰∏Ä‰∏™‰æãÂ≠êÔºåÂç≥ZERO_SHOT_REACT_DESCRIPTIONÔºåÂÆÉÁ±ª‰ºº‰∫éÈõ∂-shot ReActÈ£éÊ†ºÁöÑAgent„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb4fb12-d468-4ea1-8b4b-65e2f1ada7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents.mrkl.base import ZeroShotAgent\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0.7)\n",
    "tools = load_tools(['llm-math', 'open-meteo-api'], llm=llm)\n",
    "\n",
    "agent = ZeroShotAgent.from_llm_and_tools(\n",
    "    llm=llm,\n",
    "    tools=tools,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5224d308-7681-475c-9c5e-f983ff6f1920",
   "metadata": {},
   "source": [
    "ËØ∑Ê≥®ÊÑèÔºåLangChain‰∏≠ÁöÑ`Agent`Êú¨Ë∫´‰∏çËøêË°åÔºåÁõ∏ÂèçÔºåÂÆÉÂÆö‰πâ‰∫ÜÈÄÇÂΩìÁöÑLLM„ÄÅÂ∑•ÂÖ∑ÂíåÊèêÁ§∫ÔºåÂú®`AgentExecutor`‰∏≠ÊâßË°åÊó∂‰ΩøÁî®„ÄÇËÆ©Êàë‰ª¨ÁúãÁúã`ZeroShotAgent`ÊòØÂ¶Ç‰ΩïÊûÑÂª∫ÂÖ∂ÊèêÁ§∫ÁöÑ„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7d9616-15d7-4739-82e4-a8df23af4684",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent.llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28bea3d-908f-45f7-8885-1934563f28ae",
   "metadata": {},
   "source": [
    "Ê≥®ÊÑèÔºå`{input}` ÂÆö‰πâ‰∫ÜÁî®Êà∑ËæìÂÖ•ÊàñÈóÆÈ¢òÁöÑ‰ΩçÁΩÆÔºå‰æãÂ¶ÇÔºå‚ÄúÂì™ÊîØÁêÉÈòüËµ¢Âæó‰∫Ü2022Âπ¥ÁöÑFIFA‰∏ñÁïåÊùØÔºü‚ÄùÔºõ`{agent_scratchpad}` ÊòØ‰ª£ÁêÜÂëàÁé∞ÂÖ∂Ëøõ‰∏ÄÊ≠•ÊâßË°åÁöÑ‰∏≠Èó¥Ê≠•È™§ÁöÑ‰ΩçÁΩÆÔºå‰æãÂ¶ÇÔºåReAct‰ª£ÁêÜÁöÑ‚ÄúÊÄùËÄÉ/Âä®‰Ωú/ËßÇÂØü‚Äù‰∏âÂÖÉÁªÑÂ∫èÂàó„ÄÇÊåâËÆæËÆ°ÔºåÂú®LangChain‰∏≠ÔºåÊØè‰∏™`Agent`ÈÉΩÂ∫îËØ•Âú®ÂÖ∂ÊèêÁ§∫Ê®°Êùø‰∏≠ÂÆö‰πâ‰∏Ä‰∏™ÂèòÈáè`{agent_scratchpad}`„ÄÇ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee34599-40d1-4119-a8e5-63fb2a8e840a",
   "metadata": {},
   "source": [
    "### 3.3 AgentExecutor: Where Agents Execute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e704dc9-fc43-4e69-a41e-321cc17b6e6e",
   "metadata": {},
   "source": [
    "`AgentExecutor`ÊòØ`Agent`ÔºàÂ∞±ÂÉèÊàë‰ª¨‰∏äÈù¢ÂÆö‰πâÁöÑÈÇ£Ê†∑ÔºâÂÆûÈôÖÊâßË°åÁöÑÂú∞Êñπ„ÄÇÊ†πÊçÆÊàë‰ª¨Â∏åÊúõ‰ª£ÁêÜËøêË°åÁöÑÊñπÂºèÔºåÂèØ‰ª•Êúâ‰∏çÂêåÁ±ªÂûãÁöÑ`AgentExecutor`„ÄÇÂ§ßÂ§öÊï∞ÊÉÖÂÜµ‰∏ãÔºåÊàë‰ª¨Â∏åÊúõ‰ΩøÁî®LangChainÊèê‰æõÁöÑÈªòËÆ§`AgentExecutor`„ÄÇ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154f2be4-e2de-444d-bf65-244954077f6e",
   "metadata": {},
   "source": [
    "‰ª•‰∏ã‰ª£Á†ÅÁâáÊÆµÊù•Ëá™`AgentExecutor`ÔºåÂ±ïÁ§∫‰∫ÜLangChain‰∏≠ÈÄöÂ∏∏Â¶Ç‰ΩïÊâßË°å`Agent`„ÄÇ\n",
    "```python\n",
    "class AgentExecutor(Chain):\n",
    "    ...\n",
    "    def _call(\n",
    "        self,\n",
    "        inputs: Dict[str, str],\n",
    "        run_manager: Optional[CallbackManagerForChainRun] = None,\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Run text through and get agent response.\"\"\"\n",
    "        ...\n",
    "        # [1] To prevent `Agent`s from running into an infinite loop, `AgentExecutor` use\n",
    "        #     both number of LLM invocations (`iterations`) and used time (`time_elapsed`)\n",
    "        #     to stop execution even if `Agent` do not want to finish\n",
    "        iterations = 0\n",
    "        time_elapsed = 0.0\n",
    "        start_time = time.time()\n",
    "        # [2] We now enter into the agent loop (until it returns something).\n",
    "        while self._should_continue(iterations, time_elapsed):\n",
    "            # [3] Take a single step in the \"Thought/Action/Observation\" loop, \n",
    "            #     return either `AgentAction` plus input or `AgentFinish`\n",
    "            next_step_output = self._take_next_step(...)\n",
    "            if isinstance(next_step_output, AgentFinish):  # [4] Return if LLM decides to finish\n",
    "                return self._return(\n",
    "                    next_step_output, intermediate_steps, run_manager=run_manager\n",
    "                )\n",
    "    \n",
    "            intermediate_steps.extend(next_step_output)  # [5] Store current step, i.e, `AgentAction` plus input\n",
    "            if len(next_step_output) == 1:\n",
    "                next_step_action = next_step_output[0]\n",
    "                # See if tool should return directly\n",
    "                tool_return = self._get_tool_return(next_step_action)\n",
    "                if tool_return is not None:  # [6] Check the next `AgentAction` wants to return directly\n",
    "                    return self._return(\n",
    "                        tool_return, intermediate_steps, run_manager=run_manager\n",
    "                    )\n",
    "            iterations += 1\n",
    "            time_elapsed = time.time() - start_time\n",
    "        # [7] Deal with early stop, can still return something even if stopped in the middle\n",
    "        output = self.agent.return_stopped_response(\n",
    "            self.early_stopping_method, intermediate_steps, **inputs\n",
    "        )\n",
    "        return self._return(output, intermediate_steps, run_manager=run_manager)\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a603e894-a713-425e-8bd6-f05e3da36a55",
   "metadata": {},
   "source": [
    "### 3.4 Put It Together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155fb57e-c577-4954-a7c2-4bf07089347d",
   "metadata": {},
   "source": [
    "Áé∞Âú®ËÆ©Êàë‰ª¨Â∞Ü`Tool`„ÄÅ`Agent`Âíå`AgentExecutor`ÁªìÂêàËµ∑Êù•ÔºåÁúãÁúãLangChain‰ª£ÁêÜÊúâÂì™‰∫õÂäüËÉΩ„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8990dd5d-478d-4ebf-87f5-0a4afcb2bdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools, AgentExecutor\n",
    "from langchain.agents.mrkl.base import ZeroShotAgent\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0.7)\n",
    "tools = load_tools(['llm-math', 'open-meteo-api'], llm=llm)\n",
    "\n",
    "agent = ZeroShotAgent.from_llm_and_tools(\n",
    "    llm=llm,\n",
    "    tools=tools,\n",
    ")\n",
    "\n",
    "executor = AgentExecutor.from_agent_and_tools(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    ")\n",
    "\n",
    "print(executor.invoke('What is the weather in Berlin? Raise it to the power of 2.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce705842-df82-4862-abd2-7e44aabfc15b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
